{"cells":[{"cell_type":"markdown","metadata":{"id":"w6VLw-Z57Xxu"},"source":["# Libraries Installation"],"id":"w6VLw-Z57Xxu"},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11314,"status":"ok","timestamp":1681487990059,"user":{"displayName":"aryan sol","userId":"16168010549823356867"},"user_tz":-210},"id":"0tRlc1DK7wf-","outputId":"f1edfa65-e8a6-4a6d-90c8-6e895ebb8c5d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch_geometric\n","  Downloading torch_geometric-2.3.0.tar.gz (616 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.2/616.2 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (3.0.9)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (1.22.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (1.10.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (4.65.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (1.2.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (2.27.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (3.1.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.9/dist-packages (from torch_geometric) (5.9.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch_geometric) (2.1.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torch_geometric) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torch_geometric) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torch_geometric) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torch_geometric) (1.26.15)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch_geometric) (1.2.0)\n","Building wheels for collected packages: torch_geometric\n","  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch_geometric: filename=torch_geometric-2.3.0-py3-none-any.whl size=909897 sha256=4ef0d9795ee76ff1943f40bd4ae9295f38b5df6a5aa75ef723192ed0bcb64db7\n","  Stored in directory: /root/.cache/pip/wheels/cd/7d/6b/17150450b80b4a3656a84330e22709ccd8dc0f8f4773ba4133\n","Successfully built torch_geometric\n","Installing collected packages: torch_geometric\n","Successfully installed torch_geometric-2.3.0\n"]}],"source":["!pip install torch_geometric"],"id":"0tRlc1DK7wf-"},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11267,"status":"ok","timestamp":1681488001323,"user":{"displayName":"aryan sol","userId":"16168010549823356867"},"user_tz":-210},"id":"0fFHYJQP_QBE","outputId":"21e5f862-27e1-4670-8567-91ba3175825c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting dataset\n","  Downloading dataset-1.6.0-py2.py3-none-any.whl (18 kB)\n","Collecting alembic>=0.6.2\n","  Downloading alembic-1.10.3-py3-none-any.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.3/212.3 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sqlalchemy<2.0.0,>=1.3.2\n","  Downloading SQLAlchemy-1.4.47-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting banal>=1.0.1\n","  Downloading banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\n","Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.9/dist-packages (from alembic>=0.6.2->dataset) (4.5.0)\n","Collecting Mako\n","  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from sqlalchemy<2.0.0,>=1.3.2->dataset) (2.0.2)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.9/dist-packages (from Mako->alembic>=0.6.2->dataset) (2.1.2)\n","Installing collected packages: banal, sqlalchemy, Mako, alembic, dataset\n","  Attempting uninstall: sqlalchemy\n","    Found existing installation: SQLAlchemy 2.0.9\n","    Uninstalling SQLAlchemy-2.0.9:\n","      Successfully uninstalled SQLAlchemy-2.0.9\n","Successfully installed Mako-1.2.4 alembic-1.10.3 banal-1.0.6 dataset-1.6.0 sqlalchemy-1.4.47\n"]}],"source":["!pip install dataset"],"id":"0fFHYJQP_QBE"},{"cell_type":"markdown","metadata":{"id":"3gvLIrKsYnMo"},"source":["#Libraries Importing and Defines"],"id":"3gvLIrKsYnMo"},{"cell_type":"code","execution_count":3,"metadata":{"id":"dbbbdeb4","executionInfo":{"status":"ok","timestamp":1681488005753,"user_tz":-210,"elapsed":4442,"user":{"displayName":"aryan sol","userId":"16168010549823356867"}}},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch_geometric.data import Data\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from torch_geometric.nn import GCNConv,summary\n","import random\n","import sys\n","import copy\n","from sklearn.preprocessing import StandardScaler\n","from torch_geometric.data import HeteroData\n","from tqdm import tqdm\n","import json\n","from itertools import combinations\n","from torch_geometric.loader import DataLoader\n","import matplotlib.pyplot as plt\n","from matplotlib.colors import ListedColormap\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score\n","\n","\n","\n","#change path of the files to where data and codes are placed.\n","\n","PATH_FOLDER = \"/content/drive/MyDrive/Darmoth-Cre-Gene-Project\"\n","PATH_CRE_ATTRIBUTES = \"Dataset/LZ_cRE_attributes/LZ_chr1.txt\"\n","PATH_DNA = \"Dataset/cRE_DNA/chr1.npy\"\n","PATH_EDGE = \"Dataset/LZ_cREedge/LZ_t4_chr1.txt\"\n","\n","PORTION_HIDDEN_EDGES = 0.1\n","RATIO_NEGATIVE_SAMPLE_TO_POSITIVE = 5\n","RADIUS = 250\n","NUMBER_OF_SAMPLE_DATASET = 1000\n","RANDOM_SIZE = 50\n","NEIGHBOR_SIZE = 50\n","RATIO_TEST_SIZE = 0.2\n","FEATURE_LIST = [3,5,6,7,8,9,10,11,12,13,14]\n","BATCH_SIZE = 16\n","NUMBER_FEATURE_COMB = 11\n","GCN_INPUT_SIZE = 500\n","GCN_HIDDEN_SIZE = 300\n","GCN_OUTPUT_SIZE = 200\n","NUM_EPOCHS = 200\n","BACKWARD_LIM = 10\n","NUM_NODES_EACH_PART_PRED = 100\n","LIST_FEATURE_INDEX = [10,9,8,7,6,5,4,3,2,1,0]\n","PATH_PREDICTIONS = \"Result/newpred12_0.5.pdf\"\n","\n","torch.manual_seed(42)\n","np.random.seed(42)\n","random.seed(42)\n"],"id":"dbbbdeb4"},{"cell_type":"markdown","metadata":{"id":"LRUX8EP1Z4zy"},"source":["# Making Files and Folders Ready"],"id":"LRUX8EP1Z4zy"},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31621,"status":"ok","timestamp":1681488037368,"user":{"displayName":"aryan sol","userId":"16168010549823356867"},"user_tz":-210},"id":"Lw4R2nRUY2tK","outputId":"8d2f6da1-a6c8-49a0-e168-35e902b32196"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"id":"Lw4R2nRUY2tK"},{"cell_type":"code","execution_count":5,"metadata":{"id":"AYwav59BadXi","executionInfo":{"status":"ok","timestamp":1681488037368,"user_tz":-210,"elapsed":9,"user":{"displayName":"aryan sol","userId":"16168010549823356867"}}},"outputs":[],"source":["path = PATH_FOLDER\n","os.chdir(path)"],"id":"AYwav59BadXi"},{"cell_type":"markdown","metadata":{"id":"SOH09vp0eWf7"},"source":["# Reading Data and Generating CreDataset"],"id":"SOH09vp0eWf7"},{"cell_type":"code","execution_count":6,"metadata":{"id":"sx-LyB1hYl4l","executionInfo":{"status":"ok","timestamp":1681488037368,"user_tz":-210,"elapsed":7,"user":{"displayName":"aryan sol","userId":"16168010549823356867"}}},"outputs":[],"source":["class CreDataset:\n","\n","    def __init__(self, path_attributes, path_dna, path_edges):\n","\n","      #cRE attributes \n","      self.__read_cRE_attributes(path_attributes)\n","      self.cRE_number_of_nodes = self.cRE_attributes.size()[0]\n","      self.__read_cRE_DNA_attributes(path_dna)\n","      self.__read_cRE_is_edge(path_edges)\n","\n","\n","    def __str__(self):\n","        ans = \"\"\n","        ans += \"Number of nodes is: \" + str(self.cRE_number_of_nodes) + \"\\n\"\n","        ans += \"Number of edges is: \" + str(self.cRE_number_of_edges) + \"\\n\"\n","        ans += \"Number of DNA of each cRE is: \" + str(self.cRE_DNA.shape[1]) + \"\\n\"\n","        ans += \"Number of selected attributes of each cRE is: \" + str(self.cRE_attributes.shape[1]) + \"\\n\"\n","\n","        return ans\n","\n","\n","    def __read_cRE_attributes(self, path_attributes):\n","\n","        cRE_attributes_read = pd.read_table(path_attributes, header = None , sep=\" \")\n","        cRE_attributes_selected = cRE_attributes_read.iloc[: , FEATURE_LIST] ## why these columns? what about some feature selection algorithms?\n","        cRE_attributes_selected.to_numpy()\n","        self.cRE_attributes = torch.tensor(StandardScaler().fit_transform(cRE_attributes_selected.to_numpy()), dtype=torch.float)\n","      \n","    def __read_cRE_DNA_attributes(self, path_dna):\n","        \n","        self.cRE_DNA = np.load(path_dna)\n","\n","    def __read_cRE_is_edge(self, path_edges):\n","\n","        cRE_edge = pd.read_table(path_edges, header = None, sep=\" \")\n","        self.cRE_number_of_edges = len(cRE_edge)\n","        cRE_edge = torch.tensor([cRE_edge[:][0], cRE_edge[:][1]], dtype = torch.long)\n","\n","        self.edge_pairs = cRE_edge\n","        self.cRE_edge_matrix = 2 * torch.eye(self.cRE_number_of_nodes, self.cRE_number_of_nodes) - 1\n","\n","        for i in range(int(len(cRE_edge[1]))):\n","          self.cRE_edge_matrix[cRE_edge[0][i]][cRE_edge[1][i]] = 1\n","          self.cRE_edge_matrix[cRE_edge[1][i]][cRE_edge[0][i]] = 1\n","      \n"],"id":"sx-LyB1hYl4l"},{"cell_type":"code","execution_count":7,"metadata":{"id":"b6c73095","executionInfo":{"status":"ok","timestamp":1681488037369,"user_tz":-210,"elapsed":8,"user":{"displayName":"aryan sol","userId":"16168010549823356867"}}},"outputs":[],"source":["# import dataset\n","\n","# cre_dataset = CreDataset(PATH_CRE_ATTRIBUTES, PATH_DNA, PATH_EDGE)"],"id":"b6c73095"},{"cell_type":"code","execution_count":8,"metadata":{"id":"cO4mI6ohJ8Y6","executionInfo":{"status":"ok","timestamp":1681488037369,"user_tz":-210,"elapsed":8,"user":{"displayName":"aryan sol","userId":"16168010549823356867"}}},"outputs":[],"source":["# print(cre_dataset)"],"id":"cO4mI6ohJ8Y6"},{"cell_type":"markdown","metadata":{"id":"z1EFkq__SAGe"},"source":["# Ploting graph"],"id":"z1EFkq__SAGe"},{"cell_type":"code","execution_count":9,"metadata":{"id":"6jNaTTAgSE73","executionInfo":{"status":"ok","timestamp":1681488037369,"user_tz":-210,"elapsed":7,"user":{"displayName":"aryan sol","userId":"16168010549823356867"}}},"outputs":[],"source":["def generate_graph_pic(edges, path):\n","  plt.imsave(path, edges, cmap='bwr')"],"id":"6jNaTTAgSE73"},{"cell_type":"markdown","metadata":{"id":"6iFAFztrSGeM"},"source":["# Adding negative samples to edge matrix"],"id":"6iFAFztrSGeM"},{"cell_type":"code","execution_count":10,"metadata":{"id":"AyN3dVhcI6lu","executionInfo":{"status":"ok","timestamp":1681488037369,"user_tz":-210,"elapsed":7,"user":{"displayName":"aryan sol","userId":"16168010549823356867"}}},"outputs":[],"source":["def generate_positive_negative_matrix(portion_hidden_edges, portion_negative_to_positive, cre_dataset):\n","\n","  portion = portion_hidden_edges\n","  cRE_edge_matrix_input =  torch.clone(cre_dataset.cRE_edge_matrix)\n","  number_edge_samples = cre_dataset.cRE_number_of_edges * portion_hidden_edges\n","\n","  number_negative_samples = cre_dataset.cRE_number_of_edges * portion_negative_to_positive\n","  count_negative = 0\n","\n","  have_already_sampled = torch.eye(cre_dataset.cRE_number_of_nodes, cre_dataset.cRE_number_of_nodes)\n","\n","  num_node_zero_base = cre_dataset.cRE_number_of_nodes - 1\n","\n","  while(count_negative < number_negative_samples):\n","    \n","       edge_index_in_matrix = random.randint(0, num_node_zero_base * num_node_zero_base)\n","       node1 = edge_index_in_matrix // num_node_zero_base\n","       node2 = edge_index_in_matrix % num_node_zero_base\n","       if(cRE_edge_matrix_input[node1][node2] == -1 and have_already_sampled[node1][node2] == 0): \n","\n","          cRE_edge_matrix_input[node1][node2] = 0\n","          cRE_edge_matrix_input[node2][node1] = 0\n","          count_negative += 1\n","          have_already_sampled[node1][node2] = 1\n","          have_already_sampled[node2][node1] = 1\n","\n","  return cRE_edge_matrix_input\n"],"id":"AyN3dVhcI6lu"},{"cell_type":"code","execution_count":11,"metadata":{"id":"22d25684","executionInfo":{"status":"ok","timestamp":1681488037369,"user_tz":-210,"elapsed":7,"user":{"displayName":"aryan sol","userId":"16168010549823356867"}}},"outputs":[],"source":["# # generate positive/negative label matrix\n","# cRE_edge_matrix_input = generate_positive_negative_matrix()"],"id":"22d25684"},{"cell_type":"code","execution_count":12,"metadata":{"id":"xiebal7pSjtW","executionInfo":{"status":"ok","timestamp":1681488037369,"user_tz":-210,"elapsed":7,"user":{"displayName":"aryan sol","userId":"16168010549823356867"}}},"outputs":[],"source":["# # generate_graph_pic(cRE_edge_matrix_input, PATH_FOLDER + \"/Results/cRE-with zeros.pdf\")\n","# generate_graph_pic(cre_dataset.cRE_edge_matrix, PATH_FOLDER + \"/Result/cRE-without zeros.pdf\")"],"id":"xiebal7pSjtW"},{"cell_type":"markdown","metadata":{"id":"S7pqB9lqTb4w"},"source":["# Generate Random Subgraph"],"id":"S7pqB9lqTb4w"},{"cell_type":"code","execution_count":13,"metadata":{"id":"HY7YBbYjTf4F","executionInfo":{"status":"ok","timestamp":1681488037369,"user_tz":-210,"elapsed":7,"user":{"displayName":"aryan sol","userId":"16168010549823356867"}}},"outputs":[],"source":["def generate_random_subgraph(cre_dataset, cRE_edge_matrix_input, radius_inp, rand_inp, neighbor_inp):\n","\n","  radius = radius_inp\n","  random_size = rand_inp\n","  neighbor_size = neighbor_inp\n","  sample_size = random_size + neighbor_size\n","\n","  edge_list_node1 = []\n","  edge_list_node2 = []\n","  center = random.randint(radius, cre_dataset.cRE_number_of_nodes - radius)\n","\n","  output_expected = [-1] * (sample_size ** 2)\n","\n","  node_set1 = random.sample(range(center - radius, center + radius), k = neighbor_size)\n","  node_set2 = random.sample(list(set(range(cre_dataset.cRE_number_of_nodes)) - set(node_set1)), k = neighbor_size)\n","  node_set = node_set1 + node_set2 \n","\n","  index_node = list(range(sample_size))\n","\n","  attribute_dataset = []\n","\n","  node_part1 = random.sample((range(sample_size)), k = sample_size // 2)\n","  node_part2 = list(set((range(sample_size))) - set(node_part1))    \n","\n","  for node1 in node_part1:\n","    for node2 in node_part2: \n","      if(cRE_edge_matrix_input[node_set[node1]][node_set[node2]] == 1):\n","        edge_list_node1.extend([node1, node2])\n","        edge_list_node2.extend([node2, node1])\n","\n","      output_expected[node1 * sample_size + node2] = cRE_edge_matrix_input[node_set[node1]][node_set[node2]]\n","      output_expected[node2 * sample_size + node1] = cRE_edge_matrix_input[node_set[node2]][node_set[node1]]\n","  \n","  for indx_node in range(sample_size):\n","    attribute_dataset.append(cre_dataset.cRE_attributes[node_set[indx_node]].tolist() + cre_dataset.cRE_DNA[node_set[indx_node]].tolist())\n","\n","  edge_index = torch.tensor([edge_list_node1,edge_list_node2], dtype=torch.long)\n","\n","  return Data(x = torch.tensor(np.array(attribute_dataset)), edge_index = torch.tensor(np.array(edge_index)).long(),\n","                        y = torch.tensor(np.array(output_expected)).long())"],"id":"HY7YBbYjTf4F"},{"cell_type":"code","execution_count":14,"metadata":{"id":"6F431YZEaYuu","executionInfo":{"status":"ok","timestamp":1681488037370,"user_tz":-210,"elapsed":7,"user":{"displayName":"aryan sol","userId":"16168010549823356867"}}},"outputs":[],"source":["def generate_subgraphs(cre_dataset, cRE_edge_matrix_input, radius_inp, rand_inp, neighbor_inp, number_samples, ratio_test_size):\n","  dataset_subgraphs = []\n","\n","  for i in tqdm(range(number_samples)):\n","    new_subgraph_dataset = generate_random_subgraph(cre_dataset, cRE_edge_matrix_input, radius_inp, rand_inp, neighbor_inp, )\n","    dataset_subgraphs.append(new_subgraph_dataset)\n","  \n","  dataset_train, dataset_test = train_test_split(dataset_subgraphs, test_size = ratio_test_size, random_state = 42)\n","  return dataset_train, dataset_test"],"id":"6F431YZEaYuu"},{"cell_type":"code","execution_count":15,"metadata":{"id":"TpAqEtmUYjJ3","executionInfo":{"status":"ok","timestamp":1681488037370,"user_tz":-210,"elapsed":7,"user":{"displayName":"aryan sol","userId":"16168010549823356867"}}},"outputs":[],"source":["# dataset_train, dataset_test = generate_subgraphs(cre_dataset, cRE_edge_matrix_input, RADIUS, RANDOM_SIZE, NEIGHBOR_SIZE, \n","#                                                  NUMBER_OF_SAMPLE_DATASET, RATIO_TEST_SIZE)"],"id":"TpAqEtmUYjJ3"},{"cell_type":"markdown","metadata":{"id":"Zkt1fFfOl9IW"},"source":["# GNN Model"],"id":"Zkt1fFfOl9IW"},{"cell_type":"code","execution_count":16,"metadata":{"id":"8jGQizk9gOSa","executionInfo":{"status":"ok","timestamp":1681488037370,"user_tz":-210,"elapsed":7,"user":{"displayName":"aryan sol","userId":"16168010549823356867"}}},"outputs":[],"source":["class gnn_network(torch.nn.Module):\n","\n","    def __init__(self, index_size, gcn_input_size, gcn_hidden_size, gcn_output_size, batch_size, sample_size, cre_dataset):\n","\n","        super(gnn_network, self).__init__()\n","\n","        self.conv1 = GCNConv(gcn_input_size, gcn_hidden_size)\n","        self.conv2 = GCNConv(gcn_hidden_size, gcn_output_size)\n","\n","        self.matrix = Variable(torch.randn(gcn_output_size, gcn_output_size).type(torch.FloatTensor), requires_grad=True)\n","        self.linear = torch.nn.Linear(1,2)\n","\n","        self.bilinear = nn.Bilinear(gcn_output_size, gcn_output_size, 1)\n","\n","        self.transform1 = nn.Linear(index_size, gcn_input_size)\n","        self.transform2 = nn.Linear(768, gcn_input_size)\n","\n","        self.gcn_output_size = gcn_output_size\n","\n","        self.sample_size = sample_size\n","        self.index_size = index_size      \n","\n","    def forward(self, data):\n","\n","        x, edge_index = data.x, data.edge_index\n","        attributes1 = x[:, :self.index_size].float()\n","        attributes2 = x[:, self.index_size:].float()\n","\n","        x = self.transform1(attributes1) + self.transform2(attributes2)\n","    \n","        x = self.conv1(x.float(), edge_index)\n","  \n","        x = F.relu(x)\n","        x = F.dropout(x, training=self.training)\n","        x = self.conv2(x, edge_index)\n","\n","\n","        ypred = []\n","        for i in range(x.size(0) // self.sample_size):\n","            embedings = x[i * self.sample_size:(i + 1) * self.sample_size, :]\n","            # emb1 = torch.zeros(self.sample_size, self.sample_size, self.gcn_output_size)\n","            # emb2 = torch.zeros(self.sample_size, self.sample_size, self.gcn_output_size)\n","\n","            # emb1 = embedings.unsqueeze(0).repeat(self.sample_size, 1, 1)\n","            # emb2 = embedings.unsqueeze(1).repeat(1, self.sample_size, 1)\n","            # print(emb1)\n","            # print(emb2)\n","\n","            # input()\n","            # for i in range(self.sample_size):\n","              # for j in range(self.sample_size):\n","            # emb1 = embedings\n","            # emb2 = embedings\n","\n","            # print(emb1.shape)\n","            # print(self.bilinear(emb1, emb2).shape)\n","\n","            # input()\n","\n","            ypred.append(self.linear(torch.matmul(torch.matmul(embedings, self.matrix), embedings.t()).unsqueeze(-1)).view(-1, 2))\n","            # ypred.append(self.linear(self.bilinear(emb1, emb2)).unsqueeze(-1).view(-1, 2))\n","        return torch.stack(ypred).view(-1,2)"],"id":"8jGQizk9gOSa"},{"cell_type":"code","execution_count":36,"metadata":{"id":"REVTx0_SkPpL","executionInfo":{"status":"ok","timestamp":1681489273116,"user_tz":-210,"elapsed":909,"user":{"displayName":"aryan sol","userId":"16168010549823356867"}}},"outputs":[],"source":["def train_gnn_model(gcn_input_size, gcn_hidden_size, gcn_output_size, batch_size, sample_size, cre_dataset, \n","                    feature_inp, num_epochs, backward_lim, dataset_train):\n","  \n","  feature = torch.tensor(feature_inp)\n","  model = gnn_network(feature.size(0), gcn_input_size, gcn_hidden_size,gcn_output_size,  batch_size, sample_size, cre_dataset)\n","  criterion = torch.nn.CrossEntropyLoss(ignore_index =-1)\n","  optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n","  all_features_index = torch.cat([feature, torch.Tensor(list(range(11,779))).long()],dim=0)\n","  dataset_train_selected_att = [Data(x = torch.index_select(data.x, 1, all_features_index), edge_index = data.edge_index, y = data.y) for data in dataset_train]\n","  loader = DataLoader(dataset_train_selected_att, batch_size = batch_size, shuffle=True)\n","  losses = []\n","  for epoch in range(num_epochs):\n","    count = 0\n","    loss = 0\n","    for data in loader:\n","        pred_y = model(data)\n","        loss = loss + criterion(pred_y, data.y)\n","        count = count + 1\n","        if(count == backward_lim):\n","            loss.backward()\n","            optimizer.step()\n","            print('epoch {}, loss {}'.format(epoch, loss.item()))\n","            optimizer.zero_grad()\n","            loss = 0\n","            count = 0\n","    if(epoch==0):\n","        print(summary(model,data)) \n","    if(loss != 0):\n","        losses.append(loss.item())\n","        loss.backward()\n","        optimizer.step()\n","        print('epoch {}, loss {}'.format(epoch, loss.item()))\n","        optimizer.zero_grad()\n","  return model, losses"],"id":"REVTx0_SkPpL"},{"cell_type":"code","execution_count":18,"metadata":{"id":"ButleEcJmALB","executionInfo":{"status":"ok","timestamp":1681488037370,"user_tz":-210,"elapsed":7,"user":{"displayName":"aryan sol","userId":"16168010549823356867"}}},"outputs":[],"source":["def test_model(feature_inp, model_cRE_edge_predictior, batch_size, key_input, dataset_test):\n","\n","  feature = torch.tensor(feature_inp)\n","  all_features_index = torch.cat([feature, torch.Tensor(list(range(11,779))).long()],dim=0)\n","  dataset_test_selected_att = [Data(x = torch.index_select(d.x, 1, all_features_index), edge_index=d.edge_index, y=d.y) for d in dataset_test]\n","  loader = DataLoader(dataset_test_selected_att, batch_size = batch_size, shuffle=True)\n","\n","  y_true = []\n","  y_pred = []\n","\n","  with torch.no_grad():    \n","    for data in loader:\n","      pred_y = model_cRE_edge_predictior(data)\n","      v, i = torch.max(pred_y, dim = 1)\n","      y_true += data.y.tolist()\n","      y_pred += i\n","\n","  y_pred_samples = [y_pred[i] for i in range(len(y_pred)) if y_true[i] != -1] \n","  y_true_samples = [var for var in y_true if var != -1] \n","  f1 = f1_score(y_true_samples, y_pred_samples, average='macro')\n","  print(\"f1 score is: \", f1)      \n","\n","  return y_true, y_pred"],"id":"ButleEcJmALB"},{"cell_type":"markdown","metadata":{"id":"_TjBi-uv4E-m"},"source":["# Evaluation prediction results"],"id":"_TjBi-uv4E-m"},{"cell_type":"code","execution_count":19,"metadata":{"id":"tr6Ztq6w4OoV","executionInfo":{"status":"ok","timestamp":1681488037370,"user_tz":-210,"elapsed":7,"user":{"displayName":"aryan sol","userId":"16168010549823356867"}}},"outputs":[],"source":["def evaluate_result(y_pred, y_true):\n","  \n","  true_pos=0\n","  false_neg=0\n","  new_pos=0\n","  new_neg=0\n","  true_neg=0\n","  false_pos=0\n","\n","  for i in range(len(y_pred)):\n","      if(y_true[i] == 1):\n","          if(y_pred[i] == 1):\n","              true_pos += 1\n","          else:\n","              false_neg += 1\n","      elif(y_true[i] == -1):\n","          if(y_pred[i] == 1):\n","              new_pos += 1\n","          else:\n","              new_neg += 1\n","      elif(y_true[i] == 0):\n","          if(y_pred[i] == 1):\n","              false_pos += 1\n","          else:\n","              true_neg += 1\n","              \n","  print(\"True positive is: \", true_pos)\n","  print(\"False negative is: \", false_neg)\n","  print(\"New positive is: \", new_pos)\n","  print(\"New negative is: \", new_neg)\n","  print(\"False positive is: \", false_pos)\n","  print(\"True negative is: \", true_neg)"],"id":"tr6Ztq6w4OoV"},{"cell_type":"code","execution_count":20,"metadata":{"id":"OHvqNuIp4lYa","executionInfo":{"status":"ok","timestamp":1681488037370,"user_tz":-210,"elapsed":6,"user":{"displayName":"aryan sol","userId":"16168010549823356867"}}},"outputs":[],"source":["# evaluate_result(y_pred, y_true)"],"id":"OHvqNuIp4lYa"},{"cell_type":"markdown","metadata":{"id":"eqMOQgQo7eoT"},"source":["# Predict whole graph"],"id":"eqMOQgQo7eoT"},{"cell_type":"code","execution_count":21,"metadata":{"id":"G7tFkWy5gVpG","executionInfo":{"status":"ok","timestamp":1681488037371,"user_tz":-210,"elapsed":7,"user":{"displayName":"aryan sol","userId":"16168010549823356867"}}},"outputs":[],"source":["def predict_edges(cre_dataset, model_cRE_edge_predictor, index_subgraph1, index_subgraph2, all_attributes, num_nodes, \n","                  ):\n","  \n","  #print('cre_dataset.cRE_edge_matrix shape',cre_dataset.cRE_edge_matrix.shape)\n","  sample_size = 2 * num_nodes\n"," \n","  \n","  attributes_subgraph = torch.cat([all_attributes[index_subgraph1 * num_nodes:(index_subgraph1 + 1) * num_nodes] ,\n","                                   all_attributes[index_subgraph2 * num_nodes:(index_subgraph2 + 1) * num_nodes]],0)\n","  edge_node1, edge_node2 = (cre_dataset.cRE_edge_matrix[index_subgraph1 * num_nodes:(index_subgraph1 + 1) * num_nodes, index_subgraph2 * \n","                                                        num_nodes:(index_subgraph2 + 1) * num_nodes] == 1).nonzero(as_tuple = True)\n","  edge_index = torch.stack([edge_node1, edge_node2]).long()\n","  #print(edge_index)\n"," # input()\n","  dataset_subgraph = Data(x = attributes_subgraph, edge_index = torch.tensor(np.array(edge_index)).long())\n","  #print('edge_node1',edge_node1.shape)\n","  #print('edge_node2',edge_node2.shape)\n","  #print('esdge index shape',edge_index.shape)\n","  #print('dataset_subgraph shape num of edges',dataset_subgraph.num_edges)\n","  #print('dataset_subgraph shape num of nodes',dataset_subgraph.num_nodes)\n","  pred_y = model_cRE_edge_predictor(dataset_subgraph)\n","  #print('pred y shape is',pred_y.shape)\n","  softmax_pred = F.softmax(pred_y)\n","  v, ind = torch.max(softmax_pred, dim = 1)\n","  softmax_pred = softmax_pred.tolist()\n","\n","  diff_prob = [tupple_prob[1] - tupple_prob[0] for tupple_prob in softmax_pred]\n","\n","  is_edge = [1 if softmax_pred[k][1] - softmax_pred[k][0] >= 0.5 else 0 for k in range(len(ind))]\n","\n","  pred_edge_index = [var[0] - 1 for var in enumerate(is_edge, 1) if var[1] == 1]\n","\n","#  print(\"first\", len(pred_edge_index), pred_edge_index)\n","\n","  # just edges which are symmetric \n","  pred_edge_index = [var for var in pred_edge_index if (var % (num_nodes * 2)) * (num_nodes * 2) + (var // num_nodes * 2) in pred_edge_index]\n"," # print(\"second\", len(pred_edge_index), pred_edge_index)\n","  pred_edge_index_tmp = [var for var in pred_edge_index if (var // (num_nodes * 2) < num_nodes and var % (num_nodes * 2) >= num_nodes) or \n","                         (var // (num_nodes * 2) >= num_nodes and var % (num_nodes * 2) < num_nodes)]\n","  #print(\"third\", len(pred_edge_index), pred_edge_index)\n","\n","  pred_edge_index_tmp = [var if (var //  sample_size) < (var %  sample_size) else (var %  sample_size) * \n","                          sample_size + (var //  sample_size) for var in pred_edge_index_tmp]\n","  \n","  matrix_edges_predicted_subgraph = torch.zeros(num_nodes, num_nodes)\n","  for nodes_combined in pred_edge_index:\n","      node1 = nodes_combined // (num_nodes * 2)\n","      node2 = nodes_combined % (num_nodes * 2) - num_nodes\n","      matrix_edges_predicted_subgraph[node1, node2] = 1\n","\n","  return matrix_edges_predicted_subgraph\n","\n","  #plt.imsave(\"temp1.pdf\",tmp,cmap='bwr')\n","  #prediction[i*plotsize:(i+1)*plotsize, j*plotsize:(j+1)*plotsize] = tmp\n","\n","  #prediction_matrix[i * plotsize:(i + 1) * plotsize, j * plotsize:(j + 1) * plotsize] = matrix_edges_predicted_subgraph\n"],"id":"G7tFkWy5gVpG"},{"cell_type":"code","execution_count":22,"metadata":{"id":"uChmXonL_BeT","executionInfo":{"status":"ok","timestamp":1681488037371,"user_tz":-210,"elapsed":7,"user":{"displayName":"aryan sol","userId":"16168010549823356867"}}},"outputs":[],"source":["def predict_whole_graph(cre_dataset, model_cRE_edge_predictor, num_nodes_each_part_pred, path_out):\n","\n","  prediction = torch.zeros(10000, 10000)\n","  sample_size = 2 * NUM_NODES_EACH_PART_PRED\n","  plot_size = 100\n","  plot_size2 = 200\n","  x_attributes = copy.deepcopy(cre_dataset.cRE_attributes)\n","  x_attributes = torch.cat([x_attributes, torch.tensor(cre_dataset.cRE_DNA)], 1)\n","  x_attributes = torch.cat([x_attributes, x_attributes[:10000 - x_attributes.size(0)]])\n","\n","#  for i in tqdm(range(cre_dataset.cRE_number_of_nodes // NUM_NODES_EACH_PART_PRED + 1)):\n"," #   for j in range(0, i + 1):\n","  #    predict_edges(cre_dataset, model_cRE_edge_predictor, i, j, x_attributes, NUM_NODES_EACH_PART_PRED)\n","\n","\n","  for i in tqdm(range(cre_dataset.cRE_attributes.shape[0] // NUM_NODES_EACH_PART_PRED+1)):\n","    for j in range(0, i + 1):\n","      tmp=predict_edges(cre_dataset, model_cRE_edge_predictor, i, j, x_attributes, NUM_NODES_EACH_PART_PRED)\n","      prediction[i*plot_size:(i+1)*plot_size, j*plot_size:(j+1)*plot_size] = tmp\n","\n","  #predict_edges_old_method(cre_dataset, model_cRE_edge_predictor,  x_attributes, NUM_NODES_EACH_PART_PRED,prediction)\n","  \n","\n","  plt.imsave(path_out, prediction,cmap='bwr')\n","  torch.save(prediction, 'drepreds.pt')\n","     "],"id":"uChmXonL_BeT"},{"cell_type":"code","execution_count":23,"metadata":{"id":"h3itoc93Ahjw","executionInfo":{"status":"ok","timestamp":1681488037371,"user_tz":-210,"elapsed":7,"user":{"displayName":"aryan sol","userId":"16168010549823356867"}}},"outputs":[],"source":["# predict_whole_graph(cre_dataset, model_cRE_edge_predictor)"],"id":"h3itoc93Ahjw"},{"cell_type":"code","execution_count":24,"metadata":{"id":"bjEludXU5hDc","executionInfo":{"status":"ok","timestamp":1681488037371,"user_tz":-210,"elapsed":7,"user":{"displayName":"aryan sol","userId":"16168010549823356867"}}},"outputs":[],"source":["# ind=[1 ,1,1,0,0]\n","# #pred_edge_index = [var[0] - 1 for var in enumerate(ind,1) if var[1] == 1]\n","# for var in enumerate(ind,1):\n","#   print(\"var0\",var[0])\n","#   print(\"var1\",var[1])"],"id":"bjEludXU5hDc"},{"cell_type":"markdown","metadata":{"id":"6c5BoFNZEFD2"},"source":["# Hyper Parameters functions"],"id":"6c5BoFNZEFD2"},{"cell_type":"code","execution_count":25,"metadata":{"id":"q74yGBQSEMoO","executionInfo":{"status":"ok","timestamp":1681488037371,"user_tz":-210,"elapsed":7,"user":{"displayName":"aryan sol","userId":"16168010549823356867"}}},"outputs":[],"source":["def set_default_hyper_parameters():\n","\n","  default_hyper_parameters = {'portion_hidden_edges': PORTION_HIDDEN_EDGES, 'ratio_negative_to_positive': RATIO_NEGATIVE_SAMPLE_TO_POSITIVE, \n","                              'radius': RADIUS, 'number_of_sample_dataset': NUMBER_OF_SAMPLE_DATASET, 'random_size':  RANDOM_SIZE, \n","                              'neighbor_size': NEIGHBOR_SIZE, 'ratio_test_size': RATIO_TEST_SIZE, 'feature_selected': FEATURE_LIST, \n","                              'batch_size': BATCH_SIZE, 'number_feature_comb':  NUMBER_FEATURE_COMB, 'gcn_input_size': GCN_INPUT_SIZE, \n","                              'gcn_hidden_size': GCN_HIDDEN_SIZE, 'gcn_output_size': GCN_OUTPUT_SIZE, 'number_of_epochs': NUM_EPOCHS, \n","                              'backward_lim': BACKWARD_LIM, 'num_nodes_each_part_pred':  NUM_NODES_EACH_PART_PRED, 'list_feature_index': LIST_FEATURE_INDEX}\n","  \n","  return default_hyper_parameters"],"id":"q74yGBQSEMoO"},{"cell_type":"code","execution_count":33,"metadata":{"id":"yAqOcjMdEPi9","executionInfo":{"status":"ok","timestamp":1681489192776,"user_tz":-210,"elapsed":2,"user":{"displayName":"aryan sol","userId":"16168010549823356867"}}},"outputs":[],"source":["def set_fast_hyper_parameters():\n","\n","  default_hyper_parameters = {'portion_hidden_edges': PORTION_HIDDEN_EDGES, 'ratio_negative_to_positive': 1, \n","                              'radius': RADIUS, 'number_of_sample_dataset': 3, 'random_size':  RANDOM_SIZE, \n","                              'neighbor_size': NEIGHBOR_SIZE, 'ratio_test_size': RATIO_TEST_SIZE, 'feature_selected': FEATURE_LIST, \n","                              'batch_size': BATCH_SIZE, 'number_feature_comb':  NUMBER_FEATURE_COMB, 'gcn_input_size': GCN_INPUT_SIZE, \n","                              'gcn_hidden_size': GCN_HIDDEN_SIZE, 'gcn_output_size': GCN_OUTPUT_SIZE, 'number_of_epochs': 3, \n","                              'backward_lim': BACKWARD_LIM, 'num_nodes_each_part_pred':  NUM_NODES_EACH_PART_PRED, 'list_feature_index': LIST_FEATURE_INDEX}\n","  return default_hyper_parameters"],"id":"yAqOcjMdEPi9"},{"cell_type":"markdown","metadata":{"id":"s3TQ7NNdFh5z"},"source":["# Encapsulate functions running\n","\n"],"id":"s3TQ7NNdFh5z"},{"cell_type":"code","source":["def plot_losses(losses):\n","  plt.plot(range(1, len(losses) + 1), losses, '-o')\n","  plt.xlabel(\"epoch\")\n","  plt.ylabel(\"loss\")\n","  plt.show()"],"metadata":{"id":"fO97eVjOjQ7z","executionInfo":{"status":"ok","timestamp":1681489369983,"user_tz":-210,"elapsed":3,"user":{"displayName":"aryan sol","userId":"16168010549823356867"}}},"id":"fO97eVjOjQ7z","execution_count":42,"outputs":[]},{"cell_type":"code","execution_count":39,"metadata":{"id":"0VcW76fkFsj1","executionInfo":{"status":"ok","timestamp":1681489323947,"user_tz":-210,"elapsed":1,"user":{"displayName":"aryan sol","userId":"16168010549823356867"}}},"outputs":[],"source":["def run_prediction(hyper_paramters, path_out):\n","\n","  print(\"Loading dataset ...\\n\")\n","\n","  cre_dataset = CreDataset(PATH_CRE_ATTRIBUTES, PATH_DNA, PATH_EDGE)\n","\n","  print(\"Dataset info: \", cre_dataset)\n","  print(\"******************\")\n","\n","\n","  print(\"Creating positive negative matrix...\\n\")\n","\n","\n","\n","  cRE_edge_matrix_input = generate_positive_negative_matrix(hyper_parameters['portion_hidden_edges'], hyper_parameters['ratio_negative_to_positive'], \n","                                                            cre_dataset)\n","  \n","  print(\"******************\")\n","  print('Generating subgraphs for training ...\\n')\n","\n","\n","  dataset_train, dataset_test = generate_subgraphs(cre_dataset, cRE_edge_matrix_input, hyper_parameters['radius'], hyper_parameters['random_size'],\n","                     hyper_parameters['neighbor_size'], hyper_parameters['number_of_sample_dataset'], hyper_parameters['ratio_test_size'])\n","  \n","  print(\"\\n\\n******************\")\n","  print('Training the model ...\\n')\n","\n","  model_cRE_edge_predictor, losses = train_gnn_model(hyper_parameters['gcn_input_size'], hyper_parameters['gcn_hidden_size'],\n","                                             hyper_parameters['gcn_output_size'], hyper_parameters['batch_size'],\n","                                             hyper_parameters['neighbor_size'] + hyper_parameters['random_size'], cre_dataset, \n","                                             hyper_parameters['feature_selected'], hyper_parameters['number_of_epochs'],\n","                                             hyper_parameters['backward_lim'], dataset_train)\n","  plot_losses(losses)\n","  print(\"\\n ********************\")\n","  print(\"Testing the model ...\\n\")\n","\n","\n","  key_list=['A','B','C','D','E','F','G','H','I','J','K']\n","  key_inp = \"\".join(key_list) \n","  y_true, y_pred = test_model(hyper_parameters['feature_selected'], model_cRE_edge_predictor, hyper_parameters['batch_size'], key_inp, dataset_test)\n","\n","  print(\"\\n*********************\")\n","  print(\"Evaluation test results ... \\n\")\n","\n","  evaluate_result(y_pred, y_true)\n","\n","  print(\"\\n********************\")\n","  # print(\"Predicting whole graph edges ...\")\n","\n","  # predict_whole_graph(cre_dataset, model_cRE_edge_predictor, hyper_parameters['num_nodes_each_part_pred'], path_out)\n","\n","  print(f'Finish and the prediction output has been written in {path_out}')"],"id":"0VcW76fkFsj1"},{"cell_type":"markdown","metadata":{"id":"crgLbXGvEUTJ"},"source":["# Main Code Section"],"id":"crgLbXGvEUTJ"},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"DnTuWKqhEZqs","outputId":"f3b82bcc-e87c-456a-ef23-2bc8140799ce","executionInfo":{"status":"ok","timestamp":1681490657556,"user_tz":-210,"elapsed":1260430,"user":{"displayName":"aryan sol","userId":"16168010549823356867"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading dataset ...\n","\n","Dataset info:  Number of nodes is: 9946\n","Number of edges is: 79389\n","Number of DNA of each cRE is: 768\n","Number of selected attributes of each cRE is: 11\n","\n","******************\n","Creating positive negative matrix...\n","\n","******************\n","Generating subgraphs for training ...\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [01:46<00:00,  9.42it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","******************\n","Training the model ...\n","\n","epoch 0, loss 54.43302536010742\n","epoch 0, loss 43.823280334472656\n","epoch 0, loss 38.586212158203125\n","epoch 0, loss 36.41347885131836\n","epoch 0, loss 31.00389862060547\n","+----------------------+-----------------------+----------------+----------+\n","| Layer                | Input Shape           | Output Shape   | #Param   |\n","|----------------------+-----------------------+----------------+----------|\n","| gnn_network          | [1600, 1600]          | [160000, 2]    | 641,005  |\n","| ├─(conv1)GCNConv     | [1600, 500], [2, 526] | [1600, 300]    | 150,300  |\n","| ├─(conv2)GCNConv     | [1600, 300], [2, 526] | [1600, 200]    | 60,200   |\n","| ├─(linear)Linear     | [100, 100, 1]         | [100, 100, 2]  | 4        |\n","| ├─(bilinear)Bilinear | --                    | --             | 40,001   |\n","| ├─(transform1)Linear | [1600, 11]            | [1600, 500]    | 6,000    |\n","| ├─(transform2)Linear | [1600, 768]           | [1600, 500]    | 384,500  |\n","+----------------------+-----------------------+----------------+----------+\n","epoch 1, loss 28.086288452148438\n","epoch 1, loss 26.911203384399414\n","epoch 1, loss 21.954513549804688\n","epoch 1, loss 21.533763885498047\n","epoch 1, loss 18.684795379638672\n","epoch 2, loss 17.98190689086914\n","epoch 2, loss 17.2799072265625\n","epoch 2, loss 15.495399475097656\n","epoch 2, loss 14.324470520019531\n","epoch 2, loss 13.781366348266602\n","epoch 3, loss 13.361822128295898\n","epoch 3, loss 12.652257919311523\n","epoch 3, loss 11.182487487792969\n","epoch 3, loss 10.797582626342773\n","epoch 3, loss 9.53971004486084\n","epoch 4, loss 9.688102722167969\n","epoch 4, loss 10.004800796508789\n","epoch 4, loss 9.103729248046875\n","epoch 4, loss 8.317834854125977\n","epoch 4, loss 8.346786499023438\n","epoch 5, loss 8.117074966430664\n","epoch 5, loss 7.611232757568359\n","epoch 5, loss 7.356451511383057\n","epoch 5, loss 7.297579288482666\n","epoch 5, loss 6.961786270141602\n","epoch 6, loss 7.129212379455566\n","epoch 6, loss 7.009449005126953\n","epoch 6, loss 6.213137149810791\n","epoch 6, loss 6.339641571044922\n","epoch 6, loss 6.056020736694336\n","epoch 7, loss 5.867212772369385\n","epoch 7, loss 6.1241455078125\n","epoch 7, loss 5.819024085998535\n","epoch 7, loss 5.5244059562683105\n","epoch 7, loss 5.269328594207764\n","epoch 8, loss 5.6714701652526855\n","epoch 8, loss 5.181002616882324\n","epoch 8, loss 4.85053825378418\n","epoch 8, loss 4.927742958068848\n","epoch 8, loss 5.1422319412231445\n","epoch 9, loss 5.0040717124938965\n","epoch 9, loss 4.9254655838012695\n","epoch 9, loss 4.932991027832031\n","epoch 9, loss 4.7242207527160645\n","epoch 9, loss 4.706155776977539\n","epoch 10, loss 4.531012058258057\n","epoch 10, loss 4.395466327667236\n","epoch 10, loss 4.433276653289795\n","epoch 10, loss 4.3494768142700195\n","epoch 10, loss 4.374032974243164\n","epoch 11, loss 4.340446472167969\n","epoch 11, loss 4.291636943817139\n","epoch 11, loss 4.163372039794922\n","epoch 11, loss 4.103271484375\n","epoch 11, loss 4.013586044311523\n","epoch 12, loss 4.172159671783447\n","epoch 12, loss 3.9982128143310547\n","epoch 12, loss 3.822720766067505\n","epoch 12, loss 4.0512847900390625\n","epoch 12, loss 3.7057793140411377\n","epoch 13, loss 3.9088339805603027\n","epoch 13, loss 4.045147895812988\n","epoch 13, loss 3.593611478805542\n","epoch 13, loss 3.5994656085968018\n","epoch 13, loss 3.862537145614624\n","epoch 14, loss 3.4793572425842285\n","epoch 14, loss 3.7722768783569336\n","epoch 14, loss 3.8877475261688232\n","epoch 14, loss 3.5667543411254883\n","epoch 14, loss 3.6580286026000977\n","epoch 15, loss 3.457699775695801\n","epoch 15, loss 3.5968520641326904\n","epoch 15, loss 3.3400697708129883\n","epoch 15, loss 3.5135841369628906\n","epoch 15, loss 3.360590696334839\n","epoch 16, loss 3.4112064838409424\n","epoch 16, loss 3.6352548599243164\n","epoch 16, loss 3.2523934841156006\n","epoch 16, loss 3.401430606842041\n","epoch 16, loss 3.291975498199463\n","epoch 17, loss 3.375164747238159\n","epoch 17, loss 3.2954752445220947\n","epoch 17, loss 3.2081165313720703\n","epoch 17, loss 3.049558639526367\n","epoch 17, loss 3.250767707824707\n","epoch 18, loss 3.2362537384033203\n","epoch 18, loss 3.3048436641693115\n","epoch 18, loss 3.1760661602020264\n","epoch 18, loss 3.166849136352539\n","epoch 18, loss 2.9199957847595215\n","epoch 19, loss 3.1040592193603516\n","epoch 19, loss 2.972921848297119\n","epoch 19, loss 3.166226863861084\n","epoch 19, loss 2.952974319458008\n","epoch 19, loss 3.0177254676818848\n","epoch 20, loss 3.0867257118225098\n","epoch 20, loss 2.905756950378418\n","epoch 20, loss 3.089423656463623\n","epoch 20, loss 2.849902391433716\n","epoch 20, loss 2.9169058799743652\n","epoch 21, loss 2.9127986431121826\n","epoch 21, loss 2.872375726699829\n","epoch 21, loss 2.8564770221710205\n","epoch 21, loss 2.711992025375366\n","epoch 21, loss 2.8841915130615234\n","epoch 22, loss 2.8206324577331543\n","epoch 22, loss 2.761683940887451\n","epoch 22, loss 2.656799077987671\n","epoch 22, loss 2.692143678665161\n","epoch 22, loss 2.839694023132324\n","epoch 23, loss 2.462782859802246\n","epoch 23, loss 2.6958742141723633\n","epoch 23, loss 2.7139127254486084\n","epoch 23, loss 2.645005702972412\n","epoch 23, loss 2.718000888824463\n","epoch 24, loss 2.590735673904419\n","epoch 24, loss 2.4878172874450684\n","epoch 24, loss 2.435790538787842\n","epoch 24, loss 2.501513957977295\n","epoch 24, loss 2.540438652038574\n","epoch 25, loss 2.558011531829834\n","epoch 25, loss 2.435856819152832\n","epoch 25, loss 2.550553560256958\n","epoch 25, loss 2.387706756591797\n","epoch 25, loss 2.402568817138672\n","epoch 26, loss 2.334611415863037\n","epoch 26, loss 2.402858018875122\n","epoch 26, loss 2.3649277687072754\n","epoch 26, loss 2.3499016761779785\n","epoch 26, loss 2.481663227081299\n","epoch 27, loss 2.2897210121154785\n","epoch 27, loss 2.2896478176116943\n","epoch 27, loss 2.3987202644348145\n","epoch 27, loss 2.590360403060913\n","epoch 27, loss 2.252281665802002\n","epoch 28, loss 2.24985408782959\n","epoch 28, loss 2.312236785888672\n","epoch 28, loss 2.318283796310425\n","epoch 28, loss 2.283965587615967\n","epoch 28, loss 2.0938611030578613\n","epoch 29, loss 2.2150843143463135\n","epoch 29, loss 2.0990617275238037\n","epoch 29, loss 2.2505013942718506\n","epoch 29, loss 2.229379415512085\n","epoch 29, loss 2.2853643894195557\n","epoch 30, loss 2.1518144607543945\n","epoch 30, loss 2.1834709644317627\n","epoch 30, loss 2.120070457458496\n","epoch 30, loss 2.2596073150634766\n","epoch 30, loss 2.152510404586792\n","epoch 31, loss 1.9059410095214844\n","epoch 31, loss 2.128329277038574\n","epoch 31, loss 2.024372100830078\n","epoch 31, loss 2.11749267578125\n","epoch 31, loss 1.9994547367095947\n","epoch 32, loss 2.144101858139038\n","epoch 32, loss 2.0705416202545166\n","epoch 32, loss 2.030397891998291\n","epoch 32, loss 1.9902433156967163\n","epoch 32, loss 2.022533655166626\n","epoch 33, loss 2.0636327266693115\n","epoch 33, loss 2.0638370513916016\n","epoch 33, loss 1.9137400388717651\n","epoch 33, loss 1.9689775705337524\n","epoch 33, loss 2.0186920166015625\n","epoch 34, loss 1.906663417816162\n","epoch 34, loss 1.9123082160949707\n","epoch 34, loss 1.9724993705749512\n","epoch 34, loss 1.8745322227478027\n","epoch 34, loss 2.111996650695801\n","epoch 35, loss 1.740261197090149\n","epoch 35, loss 1.8982430696487427\n","epoch 35, loss 1.9348976612091064\n","epoch 35, loss 1.7724809646606445\n","epoch 35, loss 1.9224443435668945\n","epoch 36, loss 1.879866600036621\n","epoch 36, loss 1.7434884309768677\n","epoch 36, loss 1.8149996995925903\n","epoch 36, loss 1.7900251150131226\n","epoch 36, loss 1.849826455116272\n","epoch 37, loss 1.8649932146072388\n","epoch 37, loss 1.8034945726394653\n","epoch 37, loss 1.7157678604125977\n","epoch 37, loss 1.886927604675293\n","epoch 37, loss 1.9107919931411743\n","epoch 38, loss 1.7496862411499023\n","epoch 38, loss 1.8877454996109009\n","epoch 38, loss 1.712449550628662\n","epoch 38, loss 1.910115361213684\n","epoch 38, loss 1.764510989189148\n","epoch 39, loss 1.7658989429473877\n","epoch 39, loss 1.7641466856002808\n","epoch 39, loss 1.856215476989746\n","epoch 39, loss 1.8000496625900269\n","epoch 39, loss 1.5652189254760742\n","epoch 40, loss 1.6980255842208862\n","epoch 40, loss 1.7054091691970825\n","epoch 40, loss 1.6101592779159546\n","epoch 40, loss 1.564149260520935\n","epoch 40, loss 1.7289657592773438\n","epoch 41, loss 1.704866647720337\n","epoch 41, loss 1.7328888177871704\n","epoch 41, loss 1.7352254390716553\n","epoch 41, loss 1.6179192066192627\n","epoch 41, loss 1.5637882947921753\n","epoch 42, loss 1.6555002927780151\n","epoch 42, loss 1.6266555786132812\n","epoch 42, loss 1.5836690664291382\n","epoch 42, loss 1.4821358919143677\n","epoch 42, loss 1.6251158714294434\n","epoch 43, loss 1.58493173122406\n","epoch 43, loss 1.606258749961853\n","epoch 43, loss 1.5630967617034912\n","epoch 43, loss 1.5203670263290405\n","epoch 43, loss 1.5973254442214966\n","epoch 44, loss 1.5245532989501953\n","epoch 44, loss 1.5361645221710205\n","epoch 44, loss 1.6294118165969849\n","epoch 44, loss 1.5898981094360352\n","epoch 44, loss 1.5741682052612305\n","epoch 45, loss 1.5472608804702759\n","epoch 45, loss 1.4598655700683594\n","epoch 45, loss 1.5405426025390625\n","epoch 45, loss 1.4494975805282593\n","epoch 45, loss 1.3882887363433838\n","epoch 46, loss 1.544578194618225\n","epoch 46, loss 1.417358160018921\n","epoch 46, loss 1.495528221130371\n","epoch 46, loss 1.5909992456436157\n","epoch 46, loss 1.5319035053253174\n","epoch 47, loss 1.4994083642959595\n","epoch 47, loss 1.3649753332138062\n","epoch 47, loss 1.4725608825683594\n","epoch 47, loss 1.3798757791519165\n","epoch 47, loss 1.5405833721160889\n","epoch 48, loss 1.439432978630066\n","epoch 48, loss 1.4585072994232178\n","epoch 48, loss 1.4379534721374512\n","epoch 48, loss 1.4468363523483276\n","epoch 48, loss 1.445199966430664\n","epoch 49, loss 1.420699119567871\n","epoch 49, loss 1.4335187673568726\n","epoch 49, loss 1.4227291345596313\n","epoch 49, loss 1.3222259283065796\n","epoch 49, loss 1.3875224590301514\n","epoch 50, loss 1.3599339723587036\n","epoch 50, loss 1.3774547576904297\n","epoch 50, loss 1.3894377946853638\n","epoch 50, loss 1.3702162504196167\n","epoch 50, loss 1.392789602279663\n","epoch 51, loss 1.3748958110809326\n","epoch 51, loss 1.2824976444244385\n","epoch 51, loss 1.2865211963653564\n","epoch 51, loss 1.3870673179626465\n","epoch 51, loss 1.2475502490997314\n","epoch 52, loss 1.3986430168151855\n","epoch 52, loss 1.2519174814224243\n","epoch 52, loss 1.3720622062683105\n","epoch 52, loss 1.2880744934082031\n","epoch 52, loss 1.3164446353912354\n","epoch 53, loss 1.4252686500549316\n","epoch 53, loss 1.3056491613388062\n","epoch 53, loss 1.3186774253845215\n","epoch 53, loss 1.3118104934692383\n","epoch 53, loss 1.2409682273864746\n","epoch 54, loss 1.2885974645614624\n","epoch 54, loss 1.2429449558258057\n","epoch 54, loss 1.2769010066986084\n","epoch 54, loss 1.2276004552841187\n","epoch 54, loss 1.2398465871810913\n","epoch 55, loss 1.3282830715179443\n","epoch 55, loss 1.252773404121399\n","epoch 55, loss 1.2420161962509155\n","epoch 55, loss 1.1737706661224365\n","epoch 55, loss 1.2720842361450195\n","epoch 56, loss 1.1737123727798462\n","epoch 56, loss 1.297051191329956\n","epoch 56, loss 1.1735494136810303\n","epoch 56, loss 1.217879056930542\n","epoch 56, loss 1.1534851789474487\n","epoch 57, loss 1.2146835327148438\n","epoch 57, loss 1.2201900482177734\n","epoch 57, loss 1.27350914478302\n","epoch 57, loss 1.0720961093902588\n","epoch 57, loss 1.231723666191101\n","epoch 58, loss 1.1770482063293457\n","epoch 58, loss 1.2330201864242554\n","epoch 58, loss 1.1106846332550049\n","epoch 58, loss 1.1778842210769653\n","epoch 58, loss 1.1577038764953613\n","epoch 59, loss 1.1363564729690552\n","epoch 59, loss 1.085925817489624\n","epoch 59, loss 1.1143734455108643\n","epoch 59, loss 1.0854802131652832\n","epoch 59, loss 1.2186954021453857\n","epoch 60, loss 1.102596640586853\n","epoch 60, loss 1.0808883905410767\n","epoch 60, loss 1.253380537033081\n","epoch 60, loss 1.107596516609192\n","epoch 60, loss 1.1431288719177246\n","epoch 61, loss 1.1236547231674194\n","epoch 61, loss 1.0886017084121704\n","epoch 61, loss 1.039167046546936\n","epoch 61, loss 1.1314388513565063\n","epoch 61, loss 1.1670238971710205\n","epoch 62, loss 1.141189455986023\n","epoch 62, loss 1.0778477191925049\n","epoch 62, loss 1.0438244342803955\n","epoch 62, loss 1.019838571548462\n","epoch 62, loss 1.1501548290252686\n","epoch 63, loss 1.1001263856887817\n","epoch 63, loss 1.0746266841888428\n","epoch 63, loss 1.1210933923721313\n","epoch 63, loss 1.0804141759872437\n","epoch 63, loss 1.1101535558700562\n","epoch 64, loss 1.1276153326034546\n","epoch 64, loss 1.0575977563858032\n","epoch 64, loss 1.073570728302002\n","epoch 64, loss 1.0403543710708618\n","epoch 64, loss 1.2319976091384888\n","epoch 65, loss 1.0225855112075806\n","epoch 65, loss 1.0715488195419312\n","epoch 65, loss 1.0455079078674316\n","epoch 65, loss 1.0355435609817505\n","epoch 65, loss 1.0465656518936157\n","epoch 66, loss 0.9669820070266724\n","epoch 66, loss 1.0593485832214355\n","epoch 66, loss 1.0204291343688965\n","epoch 66, loss 1.0852365493774414\n","epoch 66, loss 1.0052781105041504\n","epoch 67, loss 1.1229649782180786\n","epoch 67, loss 1.0246734619140625\n","epoch 67, loss 1.0114777088165283\n","epoch 67, loss 0.9679274559020996\n","epoch 67, loss 0.9813435673713684\n","epoch 68, loss 0.9216775894165039\n","epoch 68, loss 1.0264089107513428\n","epoch 68, loss 0.936991810798645\n","epoch 68, loss 1.0118470191955566\n","epoch 68, loss 0.9775396585464478\n","epoch 69, loss 1.0232282876968384\n","epoch 69, loss 0.8713743686676025\n","epoch 69, loss 1.0886893272399902\n","epoch 69, loss 1.022152304649353\n","epoch 69, loss 0.9696557521820068\n","epoch 70, loss 1.0309652090072632\n","epoch 70, loss 1.0481300354003906\n","epoch 70, loss 0.8914574384689331\n","epoch 70, loss 0.9543299674987793\n","epoch 70, loss 0.9586997628211975\n","epoch 71, loss 0.8890498876571655\n","epoch 71, loss 0.935869574546814\n","epoch 71, loss 0.9803863763809204\n","epoch 71, loss 1.016921877861023\n","epoch 71, loss 1.1084719896316528\n","epoch 72, loss 0.9513804912567139\n","epoch 72, loss 0.9583138227462769\n","epoch 72, loss 0.957980215549469\n","epoch 72, loss 0.9431952238082886\n","epoch 72, loss 0.9147746562957764\n","epoch 73, loss 0.9612759351730347\n","epoch 73, loss 1.054269790649414\n","epoch 73, loss 0.9842906594276428\n","epoch 73, loss 0.9592740535736084\n","epoch 73, loss 0.9989498257637024\n","epoch 74, loss 0.9394278526306152\n","epoch 74, loss 0.9041353464126587\n","epoch 74, loss 0.8410863280296326\n","epoch 74, loss 1.0275088548660278\n","epoch 74, loss 0.8694874048233032\n","epoch 75, loss 0.981228232383728\n","epoch 75, loss 0.8236578702926636\n","epoch 75, loss 0.9572122097015381\n","epoch 75, loss 0.9303985834121704\n","epoch 75, loss 0.926468014717102\n","epoch 76, loss 0.8950557112693787\n","epoch 76, loss 0.9088554382324219\n","epoch 76, loss 0.9583027958869934\n","epoch 76, loss 0.8477139472961426\n","epoch 76, loss 0.8707882165908813\n","epoch 77, loss 0.8168507814407349\n","epoch 77, loss 0.7716671228408813\n","epoch 77, loss 0.8411595821380615\n","epoch 77, loss 0.930823564529419\n","epoch 77, loss 0.9496679306030273\n","epoch 78, loss 0.8077796697616577\n","epoch 78, loss 0.8103817701339722\n","epoch 78, loss 0.8610419631004333\n","epoch 78, loss 0.8821350336074829\n","epoch 78, loss 0.8536396026611328\n","epoch 79, loss 0.8190445303916931\n","epoch 79, loss 0.896519124507904\n","epoch 79, loss 0.8893718123435974\n","epoch 79, loss 0.8712455630302429\n","epoch 79, loss 0.7726336717605591\n","epoch 80, loss 0.8691542148590088\n","epoch 80, loss 0.8159679770469666\n","epoch 80, loss 0.917724609375\n","epoch 80, loss 0.8736477494239807\n","epoch 80, loss 0.8216989040374756\n","epoch 81, loss 0.877955436706543\n","epoch 81, loss 0.8417658805847168\n","epoch 81, loss 0.8497007489204407\n","epoch 81, loss 0.8842464685440063\n","epoch 81, loss 0.8383121490478516\n","epoch 82, loss 0.8209446668624878\n","epoch 82, loss 0.8299242854118347\n","epoch 82, loss 0.8004944324493408\n","epoch 82, loss 0.7625411748886108\n","epoch 82, loss 0.791737973690033\n","epoch 83, loss 0.8024401664733887\n","epoch 83, loss 0.8441652655601501\n","epoch 83, loss 0.7413276433944702\n","epoch 83, loss 0.8377668857574463\n","epoch 83, loss 0.8264462351799011\n","epoch 84, loss 0.7713658213615417\n","epoch 84, loss 0.7623566389083862\n","epoch 84, loss 0.8384803533554077\n","epoch 84, loss 0.8255089521408081\n","epoch 84, loss 0.8124905228614807\n","epoch 85, loss 0.7213394641876221\n","epoch 85, loss 0.7797772884368896\n","epoch 85, loss 0.8030170798301697\n","epoch 85, loss 0.8682435750961304\n","epoch 85, loss 0.7203803062438965\n","epoch 86, loss 0.6818566918373108\n","epoch 86, loss 0.8039591312408447\n","epoch 86, loss 0.810455322265625\n","epoch 86, loss 0.7674931287765503\n","epoch 86, loss 0.7510092258453369\n","epoch 87, loss 0.8246888518333435\n","epoch 87, loss 0.7619659900665283\n","epoch 87, loss 0.738914966583252\n","epoch 87, loss 0.7612076997756958\n","epoch 87, loss 0.7052637338638306\n","epoch 88, loss 0.734005868434906\n","epoch 88, loss 0.730702817440033\n","epoch 88, loss 0.723351776599884\n","epoch 88, loss 0.7662433981895447\n","epoch 88, loss 0.8744592666625977\n","epoch 89, loss 0.7336639761924744\n","epoch 89, loss 0.8275834321975708\n","epoch 89, loss 0.725214958190918\n","epoch 89, loss 0.7933152318000793\n","epoch 89, loss 0.7439547777175903\n","epoch 90, loss 0.6962518095970154\n","epoch 90, loss 0.7626122236251831\n","epoch 90, loss 0.7252317070960999\n","epoch 90, loss 0.727769136428833\n","epoch 90, loss 0.7179855108261108\n","epoch 91, loss 0.7610052824020386\n","epoch 91, loss 0.7275609374046326\n","epoch 91, loss 0.6466400623321533\n","epoch 91, loss 0.7239426970481873\n","epoch 91, loss 0.7068646550178528\n","epoch 92, loss 0.7022196650505066\n","epoch 92, loss 0.7109206914901733\n","epoch 92, loss 0.6553512811660767\n","epoch 92, loss 0.7494274973869324\n","epoch 92, loss 0.7090156078338623\n","epoch 93, loss 0.6713725328445435\n","epoch 93, loss 0.6581311225891113\n","epoch 93, loss 0.7415323257446289\n","epoch 93, loss 0.6846891641616821\n","epoch 93, loss 0.7160664796829224\n","epoch 94, loss 0.7656412124633789\n","epoch 94, loss 0.7518876194953918\n","epoch 94, loss 0.6670956015586853\n","epoch 94, loss 0.729732871055603\n","epoch 94, loss 0.7835090160369873\n","epoch 95, loss 0.716866135597229\n","epoch 95, loss 0.7030001878738403\n","epoch 95, loss 0.673872172832489\n","epoch 95, loss 0.6682273745536804\n","epoch 95, loss 0.6538853049278259\n","epoch 96, loss 0.6970469951629639\n","epoch 96, loss 0.6935944557189941\n","epoch 96, loss 0.7415964603424072\n","epoch 96, loss 0.7184921503067017\n","epoch 96, loss 0.7307342886924744\n","epoch 97, loss 0.7677274942398071\n","epoch 97, loss 0.7323037385940552\n","epoch 97, loss 0.6517703533172607\n","epoch 97, loss 0.6728515625\n","epoch 97, loss 0.7277671098709106\n","epoch 98, loss 0.7138349413871765\n","epoch 98, loss 0.679568886756897\n","epoch 98, loss 0.6439791321754456\n","epoch 98, loss 0.6592552065849304\n","epoch 98, loss 0.7256362438201904\n","epoch 99, loss 0.6662145853042603\n","epoch 99, loss 0.6178459525108337\n","epoch 99, loss 0.6631538271903992\n","epoch 99, loss 0.6569015383720398\n","epoch 99, loss 0.6460244059562683\n","epoch 100, loss 0.7127497792243958\n","epoch 100, loss 0.6962482333183289\n","epoch 100, loss 0.6819194555282593\n","epoch 100, loss 0.7007455825805664\n","epoch 100, loss 0.6226329207420349\n","epoch 101, loss 0.6799873113632202\n","epoch 101, loss 0.5875579714775085\n","epoch 101, loss 0.6571764349937439\n","epoch 101, loss 0.6947965025901794\n","epoch 101, loss 0.643129289150238\n","epoch 102, loss 0.6386463642120361\n","epoch 102, loss 0.5957652926445007\n","epoch 102, loss 0.6812873482704163\n","epoch 102, loss 0.6344478130340576\n","epoch 102, loss 0.664068877696991\n","epoch 103, loss 0.5895212888717651\n","epoch 103, loss 0.692937970161438\n","epoch 103, loss 0.6422759890556335\n","epoch 103, loss 0.5737313628196716\n","epoch 103, loss 0.6897069811820984\n","epoch 104, loss 0.6146934628486633\n","epoch 104, loss 0.6601289510726929\n","epoch 104, loss 0.5957289338111877\n","epoch 104, loss 0.6155303716659546\n","epoch 104, loss 0.7018948793411255\n","epoch 105, loss 0.60296231508255\n","epoch 105, loss 0.6725729703903198\n","epoch 105, loss 0.6264212727546692\n","epoch 105, loss 0.5986443161964417\n","epoch 105, loss 0.6698602437973022\n","epoch 106, loss 0.6461671590805054\n","epoch 106, loss 0.5534886717796326\n","epoch 106, loss 0.6357454061508179\n","epoch 106, loss 0.6355931162834167\n","epoch 106, loss 0.6342906951904297\n","epoch 107, loss 0.6862710118293762\n","epoch 107, loss 0.5547311305999756\n","epoch 107, loss 0.7142332196235657\n","epoch 107, loss 0.5813366174697876\n","epoch 107, loss 0.6743504405021667\n","epoch 108, loss 0.6247144341468811\n","epoch 108, loss 0.6170232892036438\n","epoch 108, loss 0.5943862795829773\n","epoch 108, loss 0.6555931568145752\n","epoch 108, loss 0.5839865803718567\n","epoch 109, loss 0.6230870485305786\n","epoch 109, loss 0.6097907423973083\n","epoch 109, loss 0.658745527267456\n","epoch 109, loss 0.5696408748626709\n","epoch 109, loss 0.6398212909698486\n","epoch 110, loss 0.5764669179916382\n","epoch 110, loss 0.6240299344062805\n","epoch 110, loss 0.6007722020149231\n","epoch 110, loss 0.5886067152023315\n","epoch 110, loss 0.5621444582939148\n","epoch 111, loss 0.6472492814064026\n","epoch 111, loss 0.5825669169425964\n","epoch 111, loss 0.5643841028213501\n","epoch 111, loss 0.5687481164932251\n","epoch 111, loss 0.6570844054222107\n","epoch 112, loss 0.4964161217212677\n","epoch 112, loss 0.5996167659759521\n","epoch 112, loss 0.5778328776359558\n","epoch 112, loss 0.6137486100196838\n","epoch 112, loss 0.5898295640945435\n","epoch 113, loss 0.5774959921836853\n","epoch 113, loss 0.5301474332809448\n","epoch 113, loss 0.6365702152252197\n","epoch 113, loss 0.5663524866104126\n","epoch 113, loss 0.6874127984046936\n","epoch 114, loss 0.5116945505142212\n","epoch 114, loss 0.5949225425720215\n","epoch 114, loss 0.611045777797699\n","epoch 114, loss 0.6119953393936157\n","epoch 114, loss 0.6497834324836731\n","epoch 115, loss 0.5621501803398132\n","epoch 115, loss 0.6231876611709595\n","epoch 115, loss 0.6187280416488647\n","epoch 115, loss 0.5559567809104919\n","epoch 115, loss 0.5301445126533508\n","epoch 116, loss 0.5441067218780518\n","epoch 116, loss 0.6227249503135681\n","epoch 116, loss 0.5123706459999084\n","epoch 116, loss 0.5942288637161255\n","epoch 116, loss 0.5969582200050354\n","epoch 117, loss 0.5549525022506714\n","epoch 117, loss 0.6287606954574585\n","epoch 117, loss 0.522261917591095\n","epoch 117, loss 0.5616832375526428\n","epoch 117, loss 0.5831960439682007\n","epoch 118, loss 0.5107049345970154\n","epoch 118, loss 0.5382117033004761\n","epoch 118, loss 0.4972778260707855\n","epoch 118, loss 0.557567834854126\n","epoch 118, loss 0.5927261710166931\n","epoch 119, loss 0.6041070818901062\n","epoch 119, loss 0.5849353671073914\n","epoch 119, loss 0.5954149961471558\n","epoch 119, loss 0.5181347131729126\n","epoch 119, loss 0.5391656756401062\n","epoch 120, loss 0.5573670864105225\n","epoch 120, loss 0.5193061232566833\n","epoch 120, loss 0.5269258618354797\n","epoch 120, loss 0.4840767979621887\n","epoch 120, loss 0.5384324193000793\n","epoch 121, loss 0.5930002927780151\n","epoch 121, loss 0.5940880179405212\n","epoch 121, loss 0.529435396194458\n","epoch 121, loss 0.49844858050346375\n","epoch 121, loss 0.5625849366188049\n","epoch 122, loss 0.5025942325592041\n","epoch 122, loss 0.5326153635978699\n","epoch 122, loss 0.47307273745536804\n","epoch 122, loss 0.5378656983375549\n","epoch 122, loss 0.5588631629943848\n","epoch 123, loss 0.5218315720558167\n","epoch 123, loss 0.5425848960876465\n","epoch 123, loss 0.48603931069374084\n","epoch 123, loss 0.5582817792892456\n","epoch 123, loss 0.544922411441803\n","epoch 124, loss 0.5366135239601135\n","epoch 124, loss 0.5230002403259277\n","epoch 124, loss 0.4946565628051758\n","epoch 124, loss 0.5138684511184692\n","epoch 124, loss 0.49736058712005615\n","epoch 125, loss 0.5224273800849915\n","epoch 125, loss 0.46926531195640564\n","epoch 125, loss 0.501384973526001\n","epoch 125, loss 0.5211535096168518\n","epoch 125, loss 0.5610632300376892\n","epoch 126, loss 0.4438683092594147\n","epoch 126, loss 0.5017671585083008\n","epoch 126, loss 0.49342137575149536\n","epoch 126, loss 0.5496217012405396\n","epoch 126, loss 0.5195385813713074\n","epoch 127, loss 0.47235333919525146\n","epoch 127, loss 0.46734845638275146\n","epoch 127, loss 0.5459030866622925\n","epoch 127, loss 0.5229945778846741\n","epoch 127, loss 0.5211969614028931\n","epoch 128, loss 0.4877616763114929\n","epoch 128, loss 0.49159127473831177\n","epoch 128, loss 0.4460428059101105\n","epoch 128, loss 0.4991365373134613\n","epoch 128, loss 0.4943910241127014\n","epoch 129, loss 0.48892292380332947\n","epoch 129, loss 0.43309295177459717\n","epoch 129, loss 0.5732374787330627\n","epoch 129, loss 0.46293047070503235\n","epoch 129, loss 0.46904417872428894\n","epoch 130, loss 0.46197861433029175\n","epoch 130, loss 0.5482456684112549\n","epoch 130, loss 0.5417866706848145\n","epoch 130, loss 0.4541626274585724\n","epoch 130, loss 0.5022549033164978\n","epoch 131, loss 0.45945465564727783\n","epoch 131, loss 0.45396506786346436\n","epoch 131, loss 0.4937916398048401\n","epoch 131, loss 0.5257670879364014\n","epoch 131, loss 0.5072543025016785\n","epoch 132, loss 0.4935608208179474\n","epoch 132, loss 0.5369992852210999\n","epoch 132, loss 0.5110574960708618\n","epoch 132, loss 0.5024509429931641\n","epoch 132, loss 0.48602426052093506\n","epoch 133, loss 0.4673296809196472\n","epoch 133, loss 0.516711950302124\n","epoch 133, loss 0.4782260060310364\n","epoch 133, loss 0.5270881056785583\n","epoch 133, loss 0.5437992811203003\n","epoch 134, loss 0.49742650985717773\n","epoch 134, loss 0.459309458732605\n","epoch 134, loss 0.4856865108013153\n","epoch 134, loss 0.4315628409385681\n","epoch 134, loss 0.49505677819252014\n","epoch 135, loss 0.5227603912353516\n","epoch 135, loss 0.4412577152252197\n","epoch 135, loss 0.4197590947151184\n","epoch 135, loss 0.4625259041786194\n","epoch 135, loss 0.5084460377693176\n","epoch 136, loss 0.46218639612197876\n","epoch 136, loss 0.5260108113288879\n","epoch 136, loss 0.435800701379776\n","epoch 136, loss 0.45188701152801514\n","epoch 136, loss 0.5129684209823608\n","epoch 137, loss 0.493385374546051\n","epoch 137, loss 0.4189002811908722\n","epoch 137, loss 0.43912869691848755\n","epoch 137, loss 0.4270515739917755\n","epoch 137, loss 0.49133557081222534\n","epoch 138, loss 0.42898330092430115\n","epoch 138, loss 0.4439588785171509\n","epoch 138, loss 0.47472843527793884\n","epoch 138, loss 0.45771878957748413\n","epoch 138, loss 0.45686301589012146\n","epoch 139, loss 0.46865975856781006\n","epoch 139, loss 0.5015473961830139\n","epoch 139, loss 0.4716375172138214\n","epoch 139, loss 0.45972374081611633\n","epoch 139, loss 0.4256282448768616\n","epoch 140, loss 0.452180415391922\n","epoch 140, loss 0.4642334282398224\n","epoch 140, loss 0.4855976402759552\n","epoch 140, loss 0.44476714730262756\n","epoch 140, loss 0.4331364035606384\n","epoch 141, loss 0.4464072287082672\n","epoch 141, loss 0.4378814995288849\n","epoch 141, loss 0.4493003487586975\n","epoch 141, loss 0.41019657254219055\n","epoch 141, loss 0.44996142387390137\n","epoch 142, loss 0.4153786599636078\n","epoch 142, loss 0.46053841710090637\n","epoch 142, loss 0.4447178244590759\n","epoch 142, loss 0.451521098613739\n","epoch 142, loss 0.4870985746383667\n","epoch 143, loss 0.4286624789237976\n","epoch 143, loss 0.43636152148246765\n","epoch 143, loss 0.4544481039047241\n","epoch 143, loss 0.3868843615055084\n","epoch 143, loss 0.5102242231369019\n","epoch 144, loss 0.4401831328868866\n","epoch 144, loss 0.43799927830696106\n","epoch 144, loss 0.4397052824497223\n","epoch 144, loss 0.4425254464149475\n","epoch 144, loss 0.48163503408432007\n","epoch 145, loss 0.41442233324050903\n","epoch 145, loss 0.4460367262363434\n","epoch 145, loss 0.38018742203712463\n","epoch 145, loss 0.46210816502571106\n","epoch 145, loss 0.4375752806663513\n","epoch 146, loss 0.40852463245391846\n","epoch 146, loss 0.4668671190738678\n","epoch 146, loss 0.44326481223106384\n","epoch 146, loss 0.42906272411346436\n","epoch 146, loss 0.41983988881111145\n","epoch 147, loss 0.4130783677101135\n","epoch 147, loss 0.37974652647972107\n","epoch 147, loss 0.3873513638973236\n","epoch 147, loss 0.46161216497421265\n","epoch 147, loss 0.4064224064350128\n","epoch 148, loss 0.46238452196121216\n","epoch 148, loss 0.46616891026496887\n","epoch 148, loss 0.39524349570274353\n","epoch 148, loss 0.39896124601364136\n","epoch 148, loss 0.4607664942741394\n","epoch 149, loss 0.460678368806839\n","epoch 149, loss 0.40979790687561035\n","epoch 149, loss 0.46537476778030396\n","epoch 149, loss 0.47079914808273315\n","epoch 149, loss 0.38587233424186707\n","epoch 150, loss 0.4232911765575409\n","epoch 150, loss 0.4069216847419739\n","epoch 150, loss 0.41950151324272156\n","epoch 150, loss 0.34769436717033386\n","epoch 150, loss 0.4522080421447754\n","epoch 151, loss 0.39117714762687683\n","epoch 151, loss 0.38683319091796875\n","epoch 151, loss 0.42733684182167053\n","epoch 151, loss 0.43180498480796814\n","epoch 151, loss 0.30591797828674316\n","epoch 152, loss 0.43223750591278076\n","epoch 152, loss 0.4265783727169037\n","epoch 152, loss 0.38510629534721375\n","epoch 152, loss 0.4300529360771179\n","epoch 152, loss 0.37673842906951904\n","epoch 153, loss 0.38876527547836304\n","epoch 153, loss 0.3991434574127197\n","epoch 153, loss 0.4693922996520996\n","epoch 153, loss 0.33153897523880005\n","epoch 153, loss 0.4263254404067993\n","epoch 154, loss 0.39622944593429565\n","epoch 154, loss 0.447674959897995\n","epoch 154, loss 0.4179900586605072\n","epoch 154, loss 0.39207905530929565\n","epoch 154, loss 0.4220837950706482\n","epoch 155, loss 0.3830898702144623\n","epoch 155, loss 0.42250096797943115\n","epoch 155, loss 0.44620370864868164\n","epoch 155, loss 0.40980613231658936\n","epoch 155, loss 0.40829920768737793\n","epoch 156, loss 0.42180588841438293\n","epoch 156, loss 0.4201490581035614\n","epoch 156, loss 0.41130244731903076\n","epoch 156, loss 0.3753264248371124\n","epoch 156, loss 0.3942517042160034\n","epoch 157, loss 0.3993365466594696\n","epoch 157, loss 0.3844751715660095\n","epoch 157, loss 0.35272765159606934\n","epoch 157, loss 0.38792580366134644\n","epoch 157, loss 0.4480644762516022\n","epoch 158, loss 0.35948657989501953\n","epoch 158, loss 0.3365367650985718\n","epoch 158, loss 0.37041985988616943\n","epoch 158, loss 0.43214547634124756\n","epoch 158, loss 0.39329519867897034\n","epoch 159, loss 0.3676123023033142\n","epoch 159, loss 0.378129780292511\n","epoch 159, loss 0.40305188298225403\n","epoch 159, loss 0.42655110359191895\n","epoch 159, loss 0.3821523189544678\n","epoch 160, loss 0.3207676410675049\n","epoch 160, loss 0.3769722580909729\n","epoch 160, loss 0.38747113943099976\n","epoch 160, loss 0.38355714082717896\n","epoch 160, loss 0.390350341796875\n","epoch 161, loss 0.4332868754863739\n","epoch 161, loss 0.41169849038124084\n","epoch 161, loss 0.35878196358680725\n","epoch 161, loss 0.35968124866485596\n","epoch 161, loss 0.39631572365760803\n","epoch 162, loss 0.38294026255607605\n","epoch 162, loss 0.34639087319374084\n","epoch 162, loss 0.3699623942375183\n","epoch 162, loss 0.38947397470474243\n","epoch 162, loss 0.42588895559310913\n","epoch 163, loss 0.3932948112487793\n","epoch 163, loss 0.38802415132522583\n","epoch 163, loss 0.31959277391433716\n","epoch 163, loss 0.397309809923172\n","epoch 163, loss 0.3548819422721863\n","epoch 164, loss 0.34052780270576477\n","epoch 164, loss 0.3597974181175232\n","epoch 164, loss 0.36003202199935913\n","epoch 164, loss 0.38061779737472534\n","epoch 164, loss 0.36060622334480286\n","epoch 165, loss 0.35500600934028625\n","epoch 165, loss 0.4342503249645233\n","epoch 165, loss 0.4120279848575592\n","epoch 165, loss 0.3426460325717926\n","epoch 165, loss 0.4016672372817993\n","epoch 166, loss 0.35145846009254456\n","epoch 166, loss 0.38500580191612244\n","epoch 166, loss 0.3640209436416626\n","epoch 166, loss 0.32821574807167053\n","epoch 166, loss 0.3220796287059784\n","epoch 167, loss 0.365457683801651\n","epoch 167, loss 0.3516864776611328\n","epoch 167, loss 0.3250035345554352\n","epoch 167, loss 0.3351404368877411\n","epoch 167, loss 0.359995037317276\n","epoch 168, loss 0.3536888360977173\n","epoch 168, loss 0.3530985414981842\n","epoch 168, loss 0.36485984921455383\n","epoch 168, loss 0.3571120500564575\n","epoch 168, loss 0.4005888104438782\n","epoch 169, loss 0.3789199888706207\n","epoch 169, loss 0.38122764229774475\n","epoch 169, loss 0.42848408222198486\n","epoch 169, loss 0.4060462713241577\n","epoch 169, loss 0.38143739104270935\n","epoch 170, loss 0.3521222174167633\n","epoch 170, loss 0.3581809401512146\n","epoch 170, loss 0.37144210934638977\n","epoch 170, loss 0.37311604619026184\n","epoch 170, loss 0.3735543191432953\n","epoch 171, loss 0.37331417202949524\n","epoch 171, loss 0.36274948716163635\n","epoch 171, loss 0.3445596694946289\n","epoch 171, loss 0.3876594007015228\n","epoch 171, loss 0.35340431332588196\n","epoch 172, loss 0.3534773886203766\n","epoch 172, loss 0.3671915531158447\n","epoch 172, loss 0.30975160002708435\n","epoch 172, loss 0.35507792234420776\n","epoch 172, loss 0.35692402720451355\n","epoch 173, loss 0.4214645028114319\n","epoch 173, loss 0.2846545875072479\n","epoch 173, loss 0.3385606110095978\n","epoch 173, loss 0.3088824152946472\n","epoch 173, loss 0.32934755086898804\n","epoch 174, loss 0.31345710158348083\n","epoch 174, loss 0.34972500801086426\n","epoch 174, loss 0.3169271945953369\n","epoch 174, loss 0.3195416033267975\n","epoch 174, loss 0.3756104111671448\n","epoch 175, loss 0.38207346200942993\n","epoch 175, loss 0.3406771421432495\n","epoch 175, loss 0.3183287978172302\n","epoch 175, loss 0.2711065411567688\n","epoch 175, loss 0.3371874690055847\n","epoch 176, loss 0.45565590262413025\n","epoch 176, loss 0.3612053096294403\n","epoch 176, loss 0.3807147741317749\n","epoch 176, loss 0.39552730321884155\n","epoch 176, loss 0.36101728677749634\n","epoch 177, loss 0.2929123044013977\n","epoch 177, loss 0.3317129909992218\n","epoch 177, loss 0.2918524146080017\n","epoch 177, loss 0.35648906230926514\n","epoch 177, loss 0.37469834089279175\n","epoch 178, loss 0.3244558274745941\n","epoch 178, loss 0.3428522050380707\n","epoch 178, loss 0.32716065645217896\n","epoch 178, loss 0.2950170338153839\n","epoch 178, loss 0.38206902146339417\n","epoch 179, loss 0.37024611234664917\n","epoch 179, loss 0.3395808935165405\n","epoch 179, loss 0.3299400806427002\n","epoch 179, loss 0.33519160747528076\n","epoch 179, loss 0.3284153938293457\n","epoch 180, loss 0.35358327627182007\n","epoch 180, loss 0.30545181035995483\n","epoch 180, loss 0.3422497510910034\n","epoch 180, loss 0.3533509075641632\n","epoch 180, loss 0.27295801043510437\n","epoch 181, loss 0.4093245267868042\n","epoch 181, loss 0.2861129343509674\n","epoch 181, loss 0.348944753408432\n","epoch 181, loss 0.31321942806243896\n","epoch 181, loss 0.3754465579986572\n","epoch 182, loss 0.30869072675704956\n","epoch 182, loss 0.3158753216266632\n","epoch 182, loss 0.33675897121429443\n","epoch 182, loss 0.37439972162246704\n","epoch 182, loss 0.3187083899974823\n","epoch 183, loss 0.359296590089798\n","epoch 183, loss 0.28365248441696167\n","epoch 183, loss 0.3508749306201935\n","epoch 183, loss 0.2882858216762543\n","epoch 183, loss 0.3526451289653778\n","epoch 184, loss 0.3337852358818054\n","epoch 184, loss 0.2603687644004822\n","epoch 184, loss 0.30129823088645935\n","epoch 184, loss 0.32437899708747864\n","epoch 184, loss 0.2927197813987732\n","epoch 185, loss 0.2856352925300598\n","epoch 185, loss 0.33540594577789307\n","epoch 185, loss 0.3334794342517853\n","epoch 185, loss 0.33569684624671936\n","epoch 185, loss 0.3542199730873108\n","epoch 186, loss 0.3527957499027252\n","epoch 186, loss 0.35580429434776306\n","epoch 186, loss 0.3102841377258301\n","epoch 186, loss 0.2534366250038147\n","epoch 186, loss 0.29158416390419006\n","epoch 187, loss 0.30461594462394714\n","epoch 187, loss 0.2947149872779846\n","epoch 187, loss 0.33310121297836304\n","epoch 187, loss 0.2841058075428009\n","epoch 187, loss 0.32986804842948914\n","epoch 188, loss 0.26355183124542236\n","epoch 188, loss 0.31067147850990295\n","epoch 188, loss 0.3158552944660187\n","epoch 188, loss 0.32623162865638733\n","epoch 188, loss 0.3646481931209564\n","epoch 189, loss 0.31944841146469116\n","epoch 189, loss 0.33990412950515747\n","epoch 189, loss 0.2968311011791229\n","epoch 189, loss 0.32221004366874695\n","epoch 189, loss 0.32537955045700073\n","epoch 190, loss 0.31062638759613037\n","epoch 190, loss 0.32347145676612854\n","epoch 190, loss 0.27447962760925293\n","epoch 190, loss 0.3224537968635559\n","epoch 190, loss 0.2946523427963257\n","epoch 191, loss 0.31903013586997986\n","epoch 191, loss 0.29541015625\n","epoch 191, loss 0.3134850561618805\n","epoch 191, loss 0.28723233938217163\n","epoch 191, loss 0.31605756282806396\n","epoch 192, loss 0.33800289034843445\n","epoch 192, loss 0.33139684796333313\n","epoch 192, loss 0.3066435754299164\n","epoch 192, loss 0.2696894109249115\n","epoch 192, loss 0.2727926969528198\n","epoch 193, loss 0.3234888017177582\n","epoch 193, loss 0.28433018922805786\n","epoch 193, loss 0.28359442949295044\n","epoch 193, loss 0.27340587973594666\n","epoch 193, loss 0.32628554105758667\n","epoch 194, loss 0.28467828035354614\n","epoch 194, loss 0.32577213644981384\n","epoch 194, loss 0.3021456301212311\n","epoch 194, loss 0.3093491792678833\n","epoch 194, loss 0.2866460978984833\n","epoch 195, loss 0.30024227499961853\n","epoch 195, loss 0.2948800325393677\n","epoch 195, loss 0.2809067368507385\n","epoch 195, loss 0.33394163846969604\n","epoch 195, loss 0.3801596462726593\n","epoch 196, loss 0.30853790044784546\n","epoch 196, loss 0.2566055655479431\n","epoch 196, loss 0.290202260017395\n","epoch 196, loss 0.3674275279045105\n","epoch 196, loss 0.2948687672615051\n","epoch 197, loss 0.25705623626708984\n","epoch 197, loss 0.25964951515197754\n","epoch 197, loss 0.28689348697662354\n","epoch 197, loss 0.3005906343460083\n","epoch 197, loss 0.296640545129776\n","epoch 198, loss 0.2947051227092743\n","epoch 198, loss 0.2797784209251404\n","epoch 198, loss 0.2605186998844147\n","epoch 198, loss 0.28612756729125977\n","epoch 198, loss 0.27201545238494873\n","epoch 199, loss 0.3208087086677551\n","epoch 199, loss 0.27084803581237793\n","epoch 199, loss 0.3403777480125427\n","epoch 199, loss 0.2891390025615692\n","epoch 199, loss 0.26712939143180847\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"," ********************\n","Testing the model ...\n","\n","f1 score is:  0.972352595349474\n","\n","*********************\n","Evaluation test results ... \n","\n","True positive is:  7266\n","False negative is:  210\n","New positive is:  78366\n","New negative is:  1906250\n","False positive is:  215\n","True negative is:  7693\n","\n","********************\n","Finish and the prediction output has been written in Result/prediction-cRE-whole-graph.pdf\n"]}],"source":["hyper_parameters = set_default_hyper_parameters()\n","\n","run_prediction(hyper_parameters, \"Result/prediction-cRE-whole-graph.pdf\")"],"id":"DnTuWKqhEZqs"},{"cell_type":"code","source":["z = input().split()\n","x = [float(z[i]) for i in range(len(z)) if i % 160 == 3]\n","plot_losses(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":486},"id":"0jIkrjczqTnA","executionInfo":{"status":"ok","timestamp":1681491292550,"user_tz":-210,"elapsed":16377,"user":{"displayName":"aryan sol","userId":"16168010549823356867"}},"outputId":"e0d7e5a0-ed94-4683-c09e-5e67a5d108bf"},"id":"0jIkrjczqTnA","execution_count":64,"outputs":[{"name":"stdout","output_type":"stream","text":["epoch 1, loss 28.086288452148438 epoch 1, loss 26.911203384399414 epoch 1, loss 21.954513549804688 epoch 1, loss 21.533763885498047 epoch 1, loss 18.684795379638672 epoch 2, loss 17.98190689086914 epoch 2, loss 17.2799072265625 epoch 2, loss 15.495399475097656 epoch 2, loss 14.324470520019531 epoch 2, loss 13.781366348266602 epoch 3, loss 13.361822128295898 epoch 3, loss 12.652257919311523 epoch 3, loss 11.182487487792969 epoch 3, loss 10.797582626342773 epoch 3, loss 9.53971004486084 epoch 4, loss 9.688102722167969 epoch 4, loss 10.004800796508789 epoch 4, loss 9.103729248046875 epoch 4, loss 8.317834854125977 epoch 4, loss 8.346786499023438 epoch 5, loss 8.117074966430664 epoch 5, loss 7.611232757568359 epoch 5, loss 7.356451511383057 epoch 5, loss 7.297579288482666 epoch 5, loss 6.961786270141602 epoch 6, loss 7.129212379455566 epoch 6, loss 7.009449005126953 epoch 6, loss 6.213137149810791 epoch 6, loss 6.339641571044922 epoch 6, loss 6.056020736694336 epoch 7, loss 5.867212772369385 epoch 7, loss 6.1241455078125 epoch 7, loss 5.819024085998535 epoch 7, loss 5.5244059562683105 epoch 7, loss 5.269328594207764 epoch 8, loss 5.6714701652526855 epoch 8, loss 5.181002616882324 epoch 8, loss 4.85053825378418 epoch 8, loss 4.927742958068848 epoch 8, loss 5.1422319412231445 epoch 9, loss 5.0040717124938965 epoch 9, loss 4.9254655838012695 epoch 9, loss 4.932991027832031 epoch 9, loss 4.7242207527160645 epoch 9, loss 4.706155776977539 epoch 10, loss 4.531012058258057 epoch 10, loss 4.395466327667236 epoch 10, loss 4.433276653289795 epoch 10, loss 4.3494768142700195 epoch 10, loss 4.374032974243164 epoch 11, loss 4.340446472167969 epoch 11, loss 4.291636943817139 epoch 11, loss 4.163372039794922 epoch 11, loss 4.103271484375 epoch 11, loss 4.013586044311523 epoch 12, loss 4.172159671783447 epoch 12, loss 3.9982128143310547 epoch 12, loss 3.822720766067505 epoch 12, loss 4.0512847900390625 epoch 12, loss 3.7057793140411377 epoch 13, loss 3.9088339805603027 epoch 13, loss 4.045147895812988 epoch 13, loss 3.593611478805542 epoch 13, loss 3.5994656085968018 epoch 13, loss 3.862537145614624 epoch 14, loss 3.4793572425842285 epoch 14, loss 3.7722768783569336 epoch 14, loss 3.8877475261688232 epoch 14, loss 3.5667543411254883 epoch 14, loss 3.6580286026000977 epoch 15, loss 3.457699775695801 epoch 15, loss 3.5968520641326904 epoch 15, loss 3.3400697708129883 epoch 15, loss 3.5135841369628906 epoch 15, loss 3.360590696334839 epoch 16, loss 3.4112064838409424 epoch 16, loss 3.6352548599243164 epoch 16, loss 3.2523934841156006 epoch 16, loss 3.401430606842041 epoch 16, loss 3.291975498199463 epoch 17, loss 3.375164747238159 epoch 17, loss 3.2954752445220947 epoch 17, loss 3.2081165313720703 epoch 17, loss 3.049558639526367 epoch 17, loss 3.250767707824707 epoch 18, loss 3.2362537384033203 epoch 18, loss 3.3048436641693115 epoch 18, loss 3.1760661602020264 epoch 18, loss 3.166849136352539 epoch 18, loss 2.9199957847595215 epoch 19, loss 3.1040592193603516 epoch 19, loss 2.972921848297119 epoch 19, loss 3.166226863861084 epoch 19, loss 2.952974319458008 epoch 19, loss 3.0177254676818848 epoch 20, loss 3.0867257118225098 epoch 20, loss 2.905756950378418 epoch 20, loss 3.089423656463623 epoch 20, loss 2.849902391433716 epoch 20, loss 2.9169058799743652 epoch 21, loss 2.9127986431121826 epoch 21, loss 2.872375726699829 epoch 21, loss 2.8564770221710205 epoch 21, loss 2.711992025375366 epoch 21, loss 2.8841915130615234 epoch 22, loss 2.8206324577331543 epoch 22, loss 2.761683940887451 epoch 22, loss 2.656799077987671 epoch 22, loss 2.692143678665161 epoch 22, loss 2.839694023132324 epoch 23, loss 2.462782859802246 epoch 23, loss 2.6958742141723633 epoch 23, loss 2.7139127254486084 epoch 23, loss 2.645005702972412 epoch 23, loss 2.718000888824463 epoch 24, loss 2.590735673904419 epoch 24, loss 2.4878172874450684 epoch 24, loss 2.435790538787842 epoch 24, loss 2.501513957977295 epoch 24, loss 2.540438652038574 epoch 25, loss 2.558011531829834 epoch 25, loss 2.435856819152832 epoch 25, loss 2.550553560256958 epoch 25, loss 2.387706756591797 epoch 25, loss 2.402568817138672 epoch 26, loss 2.334611415863037 epoch 26, loss 2.402858018875122 epoch 26, loss 2.3649277687072754 epoch 26, loss 2.3499016761779785 epoch 26, loss 2.481663227081299 epoch 27, loss 2.2897210121154785 epoch 27, loss 2.2896478176116943 epoch 27, loss 2.3987202644348145 epoch 27, loss 2.590360403060913 epoch 27, loss 2.252281665802002 epoch 28, loss 2.24985408782959 epoch 28, loss 2.312236785888672 epoch 28, loss 2.318283796310425 epoch 28, loss 2.283965587615967 epoch 28, loss 2.0938611030578613 epoch 29, loss 2.2150843143463135 epoch 29, loss 2.0990617275238037 epoch 29, loss 2.2505013942718506 epoch 29, loss 2.229379415512085 epoch 29, loss 2.2853643894195557 epoch 30, loss 2.1518144607543945 epoch 30, loss 2.1834709644317627 epoch 30, loss 2.120070457458496 epoch 30, loss 2.2596073150634766 epoch 30, loss 2.152510404586792 epoch 31, loss 1.9059410095214844 epoch 31, loss 2.128329277038574 epoch 31, loss 2.024372100830078 epoch 31, loss 2.11749267578125 epoch 31, loss 1.9994547367095947 epoch 32, loss 2.144101858139038 epoch 32, loss 2.0705416202545166 epoch 32, loss 2.030397891998291 epoch 32, loss 1.9902433156967163 epoch 32, loss 2.022533655166626 epoch 33, loss 2.0636327266693115 epoch 33, loss 2.0638370513916016 epoch 33, loss 1.9137400388717651 epoch 33, loss 1.9689775705337524 epoch 33, loss 2.0186920166015625 epoch 34, loss 1.906663417816162 epoch 34, loss 1.9123082160949707 epoch 34, loss 1.9724993705749512 epoch 34, loss 1.8745322227478027 epoch 34, loss 2.111996650695801 epoch 35, loss 1.740261197090149 epoch 35, loss 1.8982430696487427 epoch 35, loss 1.9348976612091064 epoch 35, loss 1.7724809646606445 epoch 35, loss 1.9224443435668945 epoch 36, loss 1.879866600036621 epoch 36, loss 1.7434884309768677 epoch 36, loss 1.8149996995925903 epoch 36, loss 1.7900251150131226 epoch 36, loss 1.849826455116272 epoch 37, loss 1.8649932146072388 epoch 37, loss 1.8034945726394653 epoch 37, loss 1.7157678604125977 epoch 37, loss 1.886927604675293 epoch 37, loss 1.9107919931411743 epoch 38, loss 1.7496862411499023 epoch 38, loss 1.8877454996109009 epoch 38, loss 1.712449550628662 epoch 38, loss 1.910115361213684 epoch 38, loss 1.764510989189148 epoch 39, loss 1.7658989429473877 epoch 39, loss 1.7641466856002808 epoch 39, loss 1.856215476989746 epoch 39, loss 1.8000496625900269 epoch 39, loss 1.5652189254760742 epoch 40, loss 1.6980255842208862 epoch 40, loss 1.7054091691970825 epoch 40, loss 1.6101592779159546 epoch 40, loss 1.564149260520935 epoch 40, loss 1.7289657592773438 epoch 41, loss 1.704866647720337 epoch 41, loss 1.7328888177871704 epoch 41, loss 1.7352254390716553 epoch 41, loss 1.6179192066192627 epoch 41, loss 1.5637882947921753 epoch 42, loss 1.6555002927780151 epoch 42, loss 1.6266555786132812 epoch 42, loss 1.5836690664291382 epoch 42, loss 1.4821358919143677 epoch 42, loss 1.6251158714294434 epoch 43, loss 1.58493173122406 epoch 43, loss 1.606258749961853 epoch 43, loss 1.5630967617034912 epoch 43, loss 1.5203670263290405 epoch 43, loss 1.5973254442214966 epoch 44, loss 1.5245532989501953 epoch 44, loss 1.5361645221710205 epoch 44, loss 1.6294118165969849 epoch 44, loss 1.5898981094360352 epoch 44, loss 1.5741682052612305 epoch 45, loss 1.5472608804702759 epoch 45, loss 1.4598655700683594 epoch 45, loss 1.5405426025390625 epoch 45, loss 1.4494975805282593 epoch 45, loss 1.3882887363433838 epoch 46, loss 1.544578194618225 epoch 46, loss 1.417358160018921 epoch 46, loss 1.495528221130371 epoch 46, loss 1.5909992456436157 epoch 46, loss 1.5319035053253174 epoch 47, loss 1.4994083642959595 epoch 47, loss 1.3649753332138062 epoch 47, loss 1.4725608825683594 epoch 47, loss 1.3798757791519165 epoch 47, loss 1.5405833721160889 epoch 48, loss 1.439432978630066 epoch 48, loss 1.4585072994232178 epoch 48, loss 1.4379534721374512 epoch 48, loss 1.4468363523483276 epoch 48, loss 1.445199966430664 epoch 49, loss 1.420699119567871 epoch 49, loss 1.4335187673568726 epoch 49, loss 1.4227291345596313 epoch 49, loss 1.3222259283065796 epoch 49, loss 1.3875224590301514 epoch 50, loss 1.3599339723587036 epoch 50, loss 1.3774547576904297 epoch 50, loss 1.3894377946853638 epoch 50, loss 1.3702162504196167 epoch 50, loss 1.392789602279663 epoch 51, loss 1.3748958110809326 epoch 51, loss 1.2824976444244385 epoch 51, loss 1.2865211963653564 epoch 51, loss 1.3870673179626465 epoch 51, loss 1.2475502490997314 epoch 52, loss 1.3986430168151855 epoch 52, loss 1.2519174814224243 epoch 52, loss 1.3720622062683105 epoch 52, loss 1.2880744934082031 epoch 52, loss 1.3164446353912354 epoch 53, loss 1.4252686500549316 epoch 53, loss 1.3056491613388062 epoch 53, loss 1.3186774253845215 epoch 53, loss 1.3118104934692383 epoch 53, loss 1.2409682273864746 epoch 54, loss 1.2885974645614624 epoch 54, loss 1.2429449558258057 epoch 54, loss 1.2769010066986084 epoch 54, loss 1.2276004552841187 epoch 54, loss 1.2398465871810913 epoch 55, loss 1.3282830715179443 epoch 55, loss 1.252773404121399 epoch 55, loss 1.2420161962509155 epoch 55, loss 1.1737706661224365 epoch 55, loss 1.2720842361450195 epoch 56, loss 1.1737123727798462 epoch 56, loss 1.297051191329956 epoch 56, loss 1.1735494136810303 epoch 56, loss 1.217879056930542 epoch 56, loss 1.1534851789474487 epoch 57, loss 1.2146835327148438 epoch 57, loss 1.2201900482177734 epoch 57, loss 1.27350914478302 epoch 57, loss 1.0720961093902588 epoch 57, loss 1.231723666191101 epoch 58, loss 1.1770482063293457 epoch 58, loss 1.2330201864242554 epoch 58, loss 1.1106846332550049 epoch 58, loss 1.1778842210769653 epoch 58, loss 1.1577038764953613 epoch 59, loss 1.1363564729690552 epoch 59, loss 1.085925817489624 epoch 59, loss 1.1143734455108643 epoch 59, loss 1.0854802131652832 epoch 59, loss 1.2186954021453857 epoch 60, loss 1.102596640586853 epoch 60, loss 1.0808883905410767 epoch 60, loss 1.253380537033081 epoch 60, loss 1.107596516609192 epoch 60, loss 1.1431288719177246 epoch 61, loss 1.1236547231674194 epoch 61, loss 1.0886017084121704 epoch 61, loss 1.039167046546936 epoch 61, loss 1.1314388513565063 epoch 61, loss 1.1670238971710205 epoch 62, loss 1.141189455986023 epoch 62, loss 1.0778477191925049 epoch 62, loss 1.0438244342803955 epoch 62, loss 1.019838571548462 epoch 62, loss 1.1501548290252686 epoch 63, loss 1.1001263856887817 epoch 63, loss 1.0746266841888428 epoch 63, loss 1.1210933923721313 epoch 63, loss 1.0804141759872437 epoch 63, loss 1.1101535558700562 epoch 64, loss 1.1276153326034546 epoch 64, loss 1.0575977563858032 epoch 64, loss 1.073570728302002 epoch 64, loss 1.0403543710708618 epoch 64, loss 1.2319976091384888 epoch 65, loss 1.0225855112075806 epoch 65, loss 1.0715488195419312 epoch 65, loss 1.0455079078674316 epoch 65, loss 1.0355435609817505 epoch 65, loss 1.0465656518936157 epoch 66, loss 0.9669820070266724 epoch 66, loss 1.0593485832214355 epoch 66, loss 1.0204291343688965 epoch 66, loss 1.0852365493774414 epoch 66, loss 1.0052781105041504 epoch 67, loss 1.1229649782180786 epoch 67, loss 1.0246734619140625 epoch 67, loss 1.0114777088165283 epoch 67, loss 0.9679274559020996 epoch 67, loss 0.9813435673713684 epoch 68, loss 0.9216775894165039 epoch 68, loss 1.0264089107513428 epoch 68, loss 0.936991810798645 epoch 68, loss 1.0118470191955566 epoch 68, loss 0.9775396585464478 epoch 69, loss 1.0232282876968384 epoch 69, loss 0.8713743686676025 epoch 69, loss 1.0886893272399902 epoch 69, loss 1.022152304649353 epoch 69, loss 0.9696557521820068 epoch 70, loss 1.0309652090072632 epoch 70, loss 1.0481300354003906 epoch 70, loss 0.8914574384689331 epoch 70, loss 0.9543299674987793 epoch 70, loss 0.9586997628211975 epoch 71, loss 0.8890498876571655 epoch 71, loss 0.935869574546814 epoch 71, loss 0.9803863763809204 epoch 71, loss 1.016921877861023 epoch 71, loss 1.1084719896316528 epoch 72, loss 0.9513804912567139 epoch 72, loss 0.9583138227462769 epoch 72, loss 0.957980215549469 epoch 72, loss 0.9431952238082886 epoch 72, loss 0.9147746562957764 epoch 73, loss 0.9612759351730347 epoch 73, loss 1.054269790649414 epoch 73, loss 0.9842906594276428 epoch 73, loss 0.9592740535736084 epoch 73, loss 0.9989498257637024 epoch 74, loss 0.9394278526306152 epoch 74, loss 0.9041353464126587 epoch 74, loss 0.8410863280296326 epoch 74, loss 1.0275088548660278 epoch 74, loss 0.8694874048233032 epoch 75, loss 0.981228232383728 epoch 75, loss 0.8236578702926636 epoch 75, loss 0.9572122097015381 epoch 75, loss 0.9303985834121704 epoch 75, loss 0.926468014717102 epoch 76, loss 0.8950557112693787 epoch 76, loss 0.9088554382324219 epoch 76, loss 0.9583027958869934 epoch 76, loss 0.8477139472961426 epoch 76, loss 0.8707882165908813 epoch 77, loss 0.8168507814407349 epoch 77, loss 0.7716671228408813 epoch 77, loss 0.8411595821380615 epoch 77, loss 0.930823564529419 epoch 77, loss 0.9496679306030273 epoch 78, loss 0.8077796697616577 epoch 78, loss 0.8103817701339722 epoch 78, loss 0.8610419631004333 epoch 78, loss 0.8821350336074829 epoch 78, loss 0.8536396026611328 epoch 79, loss 0.8190445303916931 epoch 79, loss 0.896519124507904 epoch 79, loss 0.8893718123435974 epoch 79, loss 0.8712455630302429 epoch 79, loss 0.7726336717605591 epoch 80, loss 0.8691542148590088 epoch 80, loss 0.8159679770469666 epoch 80, loss 0.917724609375 epoch 80, loss 0.8736477494239807 epoch 80, loss 0.8216989040374756 epoch 81, loss 0.877955436706543 epoch 81, loss 0.8417658805847168 epoch 81, loss 0.8497007489204407 epoch 81, loss 0.8842464685440063 epoch 81, loss 0.8383121490478516 epoch 82, loss 0.8209446668624878 epoch 82, loss 0.8299242854118347 epoch 82, loss 0.8004944324493408 epoch 82, loss 0.7625411748886108 epoch 82, loss 0.791737973690033 epoch 83, loss 0.8024401664733887 epoch 83, loss 0.8441652655601501 epoch 83, loss 0.7413276433944702 epoch 83, loss 0.8377668857574463 epoch 83, loss 0.8264462351799011 epoch 84, loss 0.7713658213615417 epoch 84, loss 0.7623566389083862 epoch 84, loss 0.8384803533554077 epoch 84, loss 0.8255089521408081 epoch 84, loss 0.8124905228614807 epoch 85, loss 0.7213394641876221 epoch 85, loss 0.7797772884368896 epoch 85, loss 0.8030170798301697 epoch 85, loss 0.8682435750961304 epoch 85, loss 0.7203803062438965 epoch 86, loss 0.6818566918373108 epoch 86, loss 0.8039591312408447 epoch 86, loss 0.810455322265625 epoch 86, loss 0.7674931287765503 epoch 86, loss 0.7510092258453369 epoch 87, loss 0.8246888518333435 epoch 87, loss 0.7619659900665283 epoch 87, loss 0.738914966583252 epoch 87, loss 0.7612076997756958 epoch 87, loss 0.7052637338638306 epoch 88, loss 0.734005868434906 epoch 88, loss 0.730702817440033 epoch 88, loss 0.723351776599884 epoch 88, loss 0.7662433981895447 epoch 88, loss 0.8744592666625977 epoch 89, loss 0.7336639761924744 epoch 89, loss 0.8275834321975708 epoch 89, loss 0.725214958190918 epoch 89, loss 0.7933152318000793 epoch 89, loss 0.7439547777175903 epoch 90, loss 0.6962518095970154 epoch 90, loss 0.7626122236251831 epoch 90, loss 0.7252317070960999 epoch 90, loss 0.727769136428833 epoch 90, loss 0.7179855108261108 epoch 91, loss 0.7610052824020386 epoch 91, loss 0.7275609374046326 epoch 91, loss 0.6466400623321533 epoch 91, loss 0.7239426970481873 epoch 91, loss 0.7068646550178528 epoch 92, loss 0.7022196650505066 epoch 92, loss 0.7109206914901733 epoch 92, loss 0.6553512811660767 epoch 92, loss 0.7494274973869324 epoch 92, loss 0.7090156078338623 epoch 93, loss 0.6713725328445435 epoch 93, loss 0.6581311225891113 epoch 93, loss 0.7415323257446289 epoch 93, loss 0.6846891641616821 epoch 93, loss 0.7160664796829224 epoch 94, loss 0.7656412124633789 epoch 94, loss 0.7518876194953918 epoch 94, loss 0.6670956015586853 epoch 94, loss 0.729732871055603 epoch 94, loss 0.7835090160369873 epoch 95, loss 0.716866135597229 epoch 95, loss 0.7030001878738403 epoch 95, loss 0.673872172832489 epoch 95, loss 0.6682273745536804 epoch 95, loss 0.6538853049278259 epoch 96, loss 0.6970469951629639 epoch 96, loss 0.6935944557189941 epoch 96, loss 0.7415964603424072 epoch 96, loss 0.7184921503067017 epoch 96, loss 0.7307342886924744 epoch 97, loss 0.7677274942398071 epoch 97, loss 0.7323037385940552 epoch 97, loss 0.6517703533172607 epoch 97, loss 0.6728515625 epoch 97, loss 0.7277671098709106 epoch 98, loss 0.7138349413871765 epoch 98, loss 0.679568886756897 epoch 98, loss 0.6439791321754456 epoch 98, loss 0.6592552065849304 epoch 98, loss 0.7256362438201904 epoch 99, loss 0.6662145853042603 epoch 99, loss 0.6178459525108337 epoch 99, loss 0.6631538271903992 epoch 99, loss 0.6569015383720398 epoch 99, loss 0.6460244059562683 epoch 100, loss 0.7127497792243958 epoch 100, loss 0.6962482333183289 epoch 100, loss 0.6819194555282593 epoch 100, loss 0.7007455825805664 epoch 100, loss 0.6226329207420349 epoch 101, loss 0.6799873113632202 epoch 101, loss 0.5875579714775085 epoch 101, loss 0.6571764349937439 epoch 101, loss 0.6947965025901794 epoch 101, loss 0.643129289150238 epoch 102, loss 0.6386463642120361 epoch 102, loss 0.5957652926445007 epoch 102, loss 0.6812873482704163 epoch 102, loss 0.6344478130340576 epoch 102, loss 0.664068877696991 epoch 103, loss 0.5895212888717651 epoch 103, loss 0.692937970161438 epoch 103, loss 0.6422759890556335 epoch 103, loss 0.5737313628196716 epoch 103, loss 0.6897069811820984 epoch 104, loss 0.6146934628486633 epoch 104, loss 0.6601289510726929 epoch 104, loss 0.5957289338111877 epoch 104, loss 0.6155303716659546 epoch 104, loss 0.7018948793411255 epoch 105, loss 0.60296231508255 epoch 105, loss 0.6725729703903198 epoch 105, loss 0.6264212727546692 epoch 105, loss 0.5986443161964417 epoch 105, loss 0.6698602437973022 epoch 106, loss 0.6461671590805054 epoch 106, loss 0.5534886717796326 epoch 106, loss 0.6357454061508179 epoch 106, loss 0.6355931162834167 epoch 106, loss 0.6342906951904297 epoch 107, loss 0.6862710118293762 epoch 107, loss 0.5547311305999756 epoch 107, loss 0.7142332196235657 epoch 107, loss 0.5813366174697876 epoch 107, loss 0.6743504405021667 epoch 108, loss 0.6247144341468811 epoch 108, loss 0.6170232892036438 epoch 108, loss 0.5943862795829773 epoch 108, loss 0.6555931568145752 epoch 108, loss 0.5839865803718567 epoch 109, loss 0.6230870485305786 epoch 109, loss 0.6097907423973083 epoch 109, loss 0.658745527267456 epoch 109, loss 0.5696408748626709 epoch 109, loss 0.6398212909698486 epoch 110, loss 0.5764669179916382 epoch 110, loss 0.6240299344062805 epoch 110, loss 0.6007722020149231 epoch 110, loss 0.5886067152023315 epoch 110, loss 0.5621444582939148 epoch 111, loss 0.6472492814064026 epoch 111, loss 0.5825669169425964 epoch 111, loss 0.5643841028213501 epoch 111, loss 0.5687481164932251 epoch 111, loss 0.6570844054222107 epoch 112, loss 0.4964161217212677 epoch 112, loss 0.5996167659759521 epoch 112, loss 0.5778328776359558 epoch 112, loss 0.6137486100196838 epoch 112, loss 0.5898295640945435 epoch 113, loss 0.5774959921836853 epoch 113, loss 0.5301474332809448 epoch 113, loss 0.6365702152252197 epoch 113, loss 0.5663524866104126 epoch 113, loss 0.6874127984046936 epoch 114, loss 0.5116945505142212 epoch 114, loss 0.5949225425720215 epoch 114, loss 0.611045777797699 epoch 114, loss 0.6119953393936157 epoch 114, loss 0.6497834324836731 epoch 115, loss 0.5621501803398132 epoch 115, loss 0.6231876611709595 epoch 115, loss 0.6187280416488647 epoch 115, loss 0.5559567809104919 epoch 115, loss 0.5301445126533508 epoch 116, loss 0.5441067218780518 epoch 116, loss 0.6227249503135681 epoch 116, loss 0.5123706459999084 epoch 116, loss 0.5942288637161255 epoch 116, loss 0.5969582200050354 epoch 117, loss 0.5549525022506714 epoch 117, loss 0.6287606954574585 epoch 117, loss 0.522261917591095 epoch 117, loss 0.5616832375526428 epoch 117, loss 0.5831960439682007 epoch 118, loss 0.5107049345970154 epoch 118, loss 0.5382117033004761 epoch 118, loss 0.4972778260707855 epoch 118, loss 0.557567834854126 epoch 118, loss 0.5927261710166931 epoch 119, loss 0.6041070818901062 epoch 119, loss 0.5849353671073914 epoch 119, loss 0.5954149961471558 epoch 119, loss 0.5181347131729126 epoch 119, loss 0.5391656756401062 epoch 120, loss 0.5573670864105225 epoch 120, loss 0.5193061232566833 epoch 120, loss 0.5269258618354797 epoch 120, loss 0.4840767979621887 epoch 120, loss 0.5384324193000793 epoch 121, loss 0.5930002927780151 epoch 121, loss 0.5940880179405212 epoch 121, loss 0.529435396194458 epoch 121, loss 0.49844858050346375 epoch 121, loss 0.5625849366188049 epoch 122, loss 0.5025942325592041 epoch 122, loss 0.5326153635978699 epoch 122, loss 0.47307273745536804 epoch 122, loss 0.5378656983375549 epoch 122, loss 0.5588631629943848 epoch 123, loss 0.5218315720558167 epoch 123, loss 0.5425848960876465 epoch 123, loss 0.48603931069374084 epoch 123, loss 0.5582817792892456 epoch 123, loss 0.544922411441803 epoch 124, loss 0.5366135239601135 epoch 124, loss 0.5230002403259277 epoch 124, loss 0.4946565628051758 epoch 124, loss 0.5138684511184692 epoch 124, loss 0.49736058712005615 epoch 125, loss 0.5224273800849915 epoch 125, loss 0.46926531195640564 epoch 125, loss 0.501384973526001 epoch 125, loss 0.5211535096168518 epoch 125, loss 0.5610632300376892 epoch 126, loss 0.4438683092594147 epoch 126, loss 0.5017671585083008 epoch 126, loss 0.49342137575149536 epoch 126, loss 0.5496217012405396 epoch 126, loss 0.5195385813713074 epoch 127, loss 0.47235333919525146 epoch 127, loss 0.46734845638275146 epoch 127, loss 0.5459030866622925 epoch 127, loss 0.5229945778846741 epoch 127, loss 0.5211969614028931 epoch 128, loss 0.4877616763114929 epoch 128, loss 0.49159127473831177 epoch 128, loss 0.4460428059101105 epoch 128, loss 0.4991365373134613 epoch 128, loss 0.4943910241127014 epoch 129, loss 0.48892292380332947 epoch 129, loss 0.43309295177459717 epoch 129, loss 0.5732374787330627 epoch 129, loss 0.46293047070503235 epoch 129, loss 0.46904417872428894 epoch 130, loss 0.46197861433029175 epoch 130, loss 0.5482456684112549 epoch 130, loss 0.5417866706848145 epoch 130, loss 0.4541626274585724 epoch 130, loss 0.5022549033164978 epoch 131, loss 0.45945465564727783 epoch 131, loss 0.45396506786346436 epoch 131, loss 0.4937916398048401 epoch 131, loss 0.5257670879364014 epoch 131, loss 0.5072543025016785 epoch 132, loss 0.4935608208179474 epoch 132, loss 0.5369992852210999 epoch 132, loss 0.5110574960708618 epoch 132, loss 0.5024509429931641 epoch 132, loss 0.48602426052093506 epoch 133, loss 0.4673296809196472 epoch 133, loss 0.516711950302124 epoch 133, loss 0.4782260060310364 epoch 133, loss 0.5270881056785583 epoch 133, loss 0.5437992811203003 epoch 134, loss 0.49742650985717773 epoch 134, loss 0.459309458732605 epoch 134, loss 0.4856865108013153 epoch 134, loss 0.4315628409385681 epoch 134, loss 0.49505677819252014 epoch 135, loss 0.5227603912353516 epoch 135, loss 0.4412577152252197 epoch 135, loss 0.4197590947151184 epoch 135, loss 0.4625259041786194 epoch 135, loss 0.5084460377693176 epoch 136, loss 0.46218639612197876 epoch 136, loss 0.5260108113288879 epoch 136, loss 0.435800701379776 epoch 136, loss 0.45188701152801514 epoch 136, loss 0.5129684209823608 epoch 137, loss 0.493385374546051 epoch 137, loss 0.4189002811908722 epoch 137, loss 0.43912869691848755 epoch 137, loss 0.4270515739917755 epoch 137, loss 0.49133557081222534 epoch 138, loss 0.42898330092430115 epoch 138, loss 0.4439588785171509 epoch 138, loss 0.47472843527793884 epoch 138, loss 0.45771878957748413 epoch 138, loss 0.45686301589012146 epoch 139, loss 0.46865975856781006 epoch 139, loss 0.5015473961830139 epoch 139, loss 0.4716375172138214 epoch 139, loss 0.45972374081611633 epoch 139, loss 0.4256282448768616 epoch 140, loss 0.452180415391922 epoch 140, loss 0.4642334282398224 epoch 140, loss 0.4855976402759552 epoch 140, loss 0.44476714730262756 epoch 140, loss 0.4331364035606384 epoch 141, loss 0.4464072287082672 epoch 141, loss 0.4378814995288849 epoch 141, loss 0.4493003487586975 epoch 141, loss 0.41019657254219055 epoch 141, loss 0.44996142387390137 epoch 142, loss 0.4153786599636078 epoch 142, loss 0.46053841710090637 epoch 142, loss 0.4447178244590759 epoch 142, loss 0.451521098613739 epoch 142, loss 0.4870985746383667 epoch 143, loss 0.4286624789237976 epoch 143, loss 0.43636152148246765 epoch 143, loss 0.4544481039047241 epoch 143, loss 0.3868843615055084 epoch 143, loss 0.5102242231369019 epoch 144, loss 0.4401831328868866 epoch 144, loss 0.43799927830696106 epoch 144, loss 0.4397052824497223 epoch 144, loss 0.4425254464149475 epoch 144, loss 0.48163503408432007 epoch 145, loss 0.41442233324050903 epoch 145, loss 0.4460367262363434 epoch 145, loss 0.38018742203712463 epoch 145, loss 0.46210816502571106 epoch 145, loss 0.4375752806663513 epoch 146, loss 0.40852463245391846 epoch 146, loss 0.4668671190738678 epoch 146, loss 0.44326481223106384 epoch 146, loss 0.42906272411346436 epoch 146, loss 0.41983988881111145 epoch 147, loss 0.4130783677101135 epoch 147, loss 0.37974652647972107 epoch 147, loss 0.3873513638973236 epoch 147, loss 0.46161216497421265 epoch 147, loss 0.4064224064350128 epoch 148, loss 0.46238452196121216 epoch 148, loss 0.46616891026496887 epoch 148, loss 0.39524349570274353 epoch 148, loss 0.39896124601364136 epoch 148, loss 0.4607664942741394 epoch 149, loss 0.460678368806839 epoch 149, loss 0.40979790687561035 epoch 149, loss 0.46537476778030396 epoch 149, loss 0.47079914808273315 epoch 149, loss 0.38587233424186707 epoch 150, loss 0.4232911765575409 epoch 150, loss 0.4069216847419739 epoch 150, loss 0.41950151324272156 epoch 150, loss 0.34769436717033386 epoch 150, loss 0.4522080421447754 epoch 151, loss 0.39117714762687683 epoch 151, loss 0.38683319091796875 epoch 151, loss 0.42733684182167053 epoch 151, loss 0.43180498480796814 epoch 151, loss 0.30591797828674316 epoch 152, loss 0.43223750591278076 epoch 152, loss 0.4265783727169037 epoch 152, loss 0.38510629534721375 epoch 152, loss 0.4300529360771179 epoch 152, loss 0.37673842906951904 epoch 153, loss 0.38876527547836304 epoch 153, loss 0.3991434574127197 epoch 153, loss 0.4693922996520996 epoch 153, loss 0.33153897523880005 epoch 153, loss 0.4263254404067993 epoch 154, loss 0.39622944593429565 epoch 154, loss 0.447674959897995 epoch 154, loss 0.4179900586605072 epoch 154, loss 0.39207905530929565 epoch 154, loss 0.4220837950706482 epoch 155, loss 0.3830898702144623 epoch 155, loss 0.42250096797943115 epoch 155, loss 0.44620370864868164 epoch 155, loss 0.40980613231658936 epoch 155, loss 0.40829920768737793 epoch 156, loss 0.42180588841438293 epoch 156, loss 0.4201490581035614 epoch 156, loss 0.41130244731903076 epoch 156, loss 0.3753264248371124 epoch 156, loss 0.3942517042160034 epoch 157, loss 0.3993365466594696 epoch 157, loss 0.3844751715660095 epoch 157, loss 0.35272765159606934 epoch 157, loss 0.38792580366134644 epoch 157, loss 0.4480644762516022 epoch 158, loss 0.35948657989501953 epoch 158, loss 0.3365367650985718 epoch 158, loss 0.37041985988616943 epoch 158, loss 0.43214547634124756 epoch 158, loss 0.39329519867897034 epoch 159, loss 0.3676123023033142 epoch 159, loss 0.378129780292511 epoch 159, loss 0.40305188298225403 epoch 159, loss 0.42655110359191895 epoch 159, loss 0.3821523189544678 epoch 160, loss 0.3207676410675049 epoch 160, loss 0.3769722580909729 epoch 160, loss 0.38747113943099976 epoch 160, loss 0.38355714082717896 epoch 160, loss 0.390350341796875 epoch 161, loss 0.4332868754863739 epoch 161, loss 0.41169849038124084 epoch 161, loss 0.35878196358680725 epoch 161, loss 0.35968124866485596 epoch 161, loss 0.39631572365760803 epoch 162, loss 0.38294026255607605 epoch 162, loss 0.34639087319374084 epoch 162, loss 0.3699623942375183 epoch 162, loss 0.38947397470474243 epoch 162, loss 0.42588895559310913 epoch 163, loss 0.3932948112487793 epoch 163, loss 0.38802415132522583 epoch 163, loss 0.31959277391433716 epoch 163, loss 0.397309809923172 epoch 163, loss 0.3548819422721863 epoch 164, loss 0.34052780270576477 epoch 164, loss 0.3597974181175232 epoch 164, loss 0.36003202199935913 epoch 164, loss 0.38061779737472534 epoch 164, loss 0.36060622334480286 epoch 165, loss 0.35500600934028625 epoch 165, loss 0.4342503249645233 epoch 165, loss 0.4120279848575592 epoch 165, loss 0.3426460325717926 epoch 165, loss 0.4016672372817993 epoch 166, loss 0.35145846009254456 epoch 166, loss 0.38500580191612244 epoch 166, loss 0.3640209436416626 epoch 166, loss 0.32821574807167053 epoch 166, loss 0.3220796287059784 epoch 167, loss 0.365457683801651 epoch 167, loss 0.3516864776611328 epoch 167, loss 0.3250035345554352 epoch 167, loss 0.3351404368877411 epoch 167, loss 0.359995037317276 epoch 168, loss 0.3536888360977173 epoch 168, loss 0.3530985414981842 epoch 168, loss 0.36485984921455383 epoch 168, loss 0.3571120500564575 epoch 168, loss 0.4005888104438782 epoch 169, loss 0.3789199888706207 epoch 169, loss 0.38122764229774475 epoch 169, loss 0.42848408222198486 epoch 169, loss 0.4060462713241577 epoch 169, loss 0.38143739104270935 epoch 170, loss 0.3521222174167633 epoch 170, loss 0.3581809401512146 epoch 170, loss 0.37144210934638977 epoch 170, loss 0.37311604619026184 epoch 170, loss 0.3735543191432953 epoch 171, loss 0.37331417202949524 epoch 171, loss 0.36274948716163635 epoch 171, loss 0.3445596694946289 epoch 171, loss 0.3876594007015228 epoch 171, loss 0.35340431332588196 epoch 172, loss 0.3534773886203766 epoch 172, loss 0.3671915531158447 epoch 172, loss 0.30975160002708435 epoch 172, loss 0.35507792234420776 epoch 172, loss 0.35692402720451355 epoch 173, loss 0.4214645028114319 epoch 173, loss 0.2846545875072479 epoch 173, loss 0.3385606110095978 epoch 173, loss 0.3088824152946472 epoch 173, loss 0.32934755086898804 epoch 174, loss 0.31345710158348083 epoch 174, loss 0.34972500801086426 epoch 174, loss 0.3169271945953369 epoch 174, loss 0.3195416033267975 epoch 174, loss 0.3756104111671448 epoch 175, loss 0.38207346200942993 epoch 175, loss 0.3406771421432495 epoch 175, loss 0.3183287978172302 epoch 175, loss 0.2711065411567688 epoch 175, loss 0.3371874690055847 epoch 176, loss 0.45565590262413025 epoch 176, loss 0.3612053096294403 epoch 176, loss 0.3807147741317749 epoch 176, loss 0.39552730321884155 epoch 176, loss 0.36101728677749634 epoch 177, loss 0.2929123044013977 epoch 177, loss 0.3317129909992218 epoch 177, loss 0.2918524146080017 epoch 177, loss 0.35648906230926514 epoch 177, loss 0.37469834089279175 epoch 178, loss 0.3244558274745941 epoch 178, loss 0.3428522050380707 epoch 178, loss 0.32716065645217896 epoch 178, loss 0.2950170338153839 epoch 178, loss 0.38206902146339417 epoch 179, loss 0.37024611234664917 epoch 179, loss 0.3395808935165405 epoch 179, loss 0.3299400806427002 epoch 179, loss 0.33519160747528076 epoch 179, loss 0.3284153938293457 epoch 180, loss 0.35358327627182007 epoch 180, loss 0.30545181035995483 epoch 180, loss 0.3422497510910034 epoch 180, loss 0.3533509075641632 epoch 180, loss 0.27295801043510437 epoch 181, loss 0.4093245267868042 epoch 181, loss 0.2861129343509674 epoch 181, loss 0.348944753408432 epoch 181, loss 0.31321942806243896 epoch 181, loss 0.3754465579986572 epoch 182, loss 0.30869072675704956 epoch 182, loss 0.3158753216266632 epoch 182, loss 0.33675897121429443 epoch 182, loss 0.37439972162246704 epoch 182, loss 0.3187083899974823 epoch 183, loss 0.359296590089798 epoch 183, loss 0.28365248441696167 epoch 183, loss 0.3508749306201935 epoch 183, loss 0.2882858216762543 epoch 183, loss 0.3526451289653778 epoch 184, loss 0.3337852358818054 epoch 184, loss 0.2603687644004822 epoch 184, loss 0.30129823088645935 epoch 184, loss 0.32437899708747864 epoch 184, loss 0.2927197813987732 epoch 185, loss 0.2856352925300598 epoch 185, loss 0.33540594577789307 epoch 185, loss 0.3334794342517853 epoch 185, loss 0.33569684624671936 epoch 185, loss 0.3542199730873108 epoch 186, loss 0.3527957499027252 epoch 186, loss 0.35580429434776306 epoch 186, loss 0.3102841377258301 epoch 186, loss 0.2534366250038147 epoch 186, loss 0.29158416390419006 epoch 187, loss 0.30461594462394714 epoch 187, loss 0.2947149872779846 epoch 187, loss 0.33310121297836304 epoch 187, loss 0.2841058075428009 epoch 187, loss 0.32986804842948914 epoch 188, loss 0.26355183124542236 epoch 188, loss 0.31067147850990295 epoch 188, loss 0.3158552944660187 epoch 188, loss 0.32623162865638733 epoch 188, loss 0.3646481931209564 epoch 189, loss 0.31944841146469116 epoch 189, loss 0.33990412950515747 epoch 189, loss 0.2968311011791229 epoch 189, loss 0.32221004366874695 epoch 189, loss 0.32537955045700073 epoch 190, loss 0.31062638759613037 epoch 190, loss 0.32347145676612854 epoch 190, loss 0.27447962760925293 epoch 190, loss 0.3224537968635559 epoch 190, loss 0.2946523427963257 epoch 191, loss 0.31903013586997986 epoch 191, loss 0.29541015625 epoch 191, loss 0.3134850561618805 epoch 191, loss 0.28723233938217163 epoch 191, loss 0.31605756282806396 epoch 192, loss 0.33800289034843445 epoch 192, loss 0.33139684796333313 epoch 192, loss 0.3066435754299164 epoch 192, loss 0.2696894109249115 epoch 192, loss 0.2727926969528198 epoch 193, loss 0.3234888017177582 epoch 193, loss 0.28433018922805786 epoch 193, loss 0.28359442949295044 epoch 193, loss 0.27340587973594666 epoch 193, loss 0.32628554105758667 epoch 194, loss 0.28467828035354614 epoch 194, loss 0.32577213644981384 epoch 194, loss 0.3021456301212311 epoch 194, loss 0.3093491792678833 epoch 194, loss 0.2866460978984833 epoch 195, loss 0.30024227499961853 epoch 195, loss 0.2948800325393677 epoch 195, loss 0.2809067368507385 epoch 195, loss 0.33394163846969604 epoch 195, loss 0.3801596462726593 epoch 196, loss 0.30853790044784546 epoch 196, loss 0.2566055655479431 epoch 196, loss 0.290202260017395 epoch 196, loss 0.3674275279045105 epoch 196, loss 0.2948687672615051 epoch 197, loss 0.25705623626708984 epoch 197, loss 0.25964951515197754 epoch 197, loss 0.28689348697662354 epoch 197, loss 0.3005906343460083 epoch 197, loss 0.296640545129776 epoch 198, loss 0.2947051227092743 epoch 198, loss 0.2797784209251404 epoch 198, loss 0.2605186998844147 epoch 198, loss 0.28612756729125977 epoch 198, loss 0.27201545238494873 epoch 199, loss 0.3208087086677551 epoch 199, loss 0.27084803581237793 epoch 199, loss 0.3403777480125427 epoch 199, loss 0.2891390025615692 epoch 199, loss 0.26712939143180847\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":165},"id":"Mf7Pxeo4p3ji","executionInfo":{"status":"error","timestamp":1681490695137,"user_tz":-210,"elapsed":1328,"user":{"displayName":"aryan sol","userId":"16168010549823356867"}},"outputId":"0005c9ac-f0b9-4dd5-f8fa-fd0a8322abcf"},"id":"Mf7Pxeo4p3ji","execution_count":46,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-46-2cb65e6279e4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"iJOM_pZOBvbk"},"source":["# Just For Testing Different methods"],"id":"iJOM_pZOBvbk"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tf6yF1KNcVUF"},"outputs":[],"source":["def set_test_hyper_parameters():\n","\n","  default_hyper_parameters = {'portion_hidden_edges': PORTION_HIDDEN_EDGES, 'ratio_negative_to_positive': 3, \n","                              'radius': RADIUS, 'number_of_sample_dataset': 100, 'random_size':  RANDOM_SIZE, \n","                              'neighbor_size': NEIGHBOR_SIZE, 'ratio_test_size': RATIO_TEST_SIZE, 'feature_selected': FEATURE_LIST, \n","                              'batch_size': BATCH_SIZE, 'number_feature_comb':  NUMBER_FEATURE_COMB, 'gcn_input_size': GCN_INPUT_SIZE, \n","                              'gcn_hidden_size': GCN_HIDDEN_SIZE, 'gcn_output_size': GCN_OUTPUT_SIZE, 'number_of_epochs': 20, \n","                              'backward_lim': BACKWARD_LIM, 'num_nodes_each_part_pred':  NUM_NODES_EACH_PART_PRED, 'list_feature_index': LIST_FEATURE_INDEX}\n","  return default_hyper_parameters"],"id":"Tf6yF1KNcVUF"},{"cell_type":"code","execution_count":null,"metadata":{"id":"SW9_H-zWdRPl"},"outputs":[],"source":["class gnn_network(torch.nn.Module):\n","\n","    def __init__(self, index_size, gcn_input_size, gcn_hidden_size, gcn_output_size, batch_size, sample_size, cre_dataset):\n","\n","        super(gnn_network, self).__init__()\n","\n","        self.conv1 = GCNConv(gcn_input_size, gcn_hidden_size)\n","        self.conv2 = GCNConv(gcn_hidden_size, gcn_output_size)\n","\n","\n","        # self.conv_embedings1 = nn.Conv2d(self.sample_size, 200, 5)\n","        # self.conv_embedings2 = nn.Conv2d(6, 16, 5)\n","\n","        self.matrix = Variable(torch.randn(gcn_output_size, gcn_output_size).type(torch.FloatTensor), requires_grad=True)\n","        self.linear = torch.nn.Linear(1,2)\n","\n","        self.bilinear = nn.Bilinear(20, 30, 40)\n","\n","        self.transform1 = nn.Linear(index_size, gcn_input_size)\n","        self.transform2 = nn.Linear(768, gcn_input_size)\n","\n","        self.sample_size = sample_size\n","        self.index_size = index_size      \n","\n","    def forward(self, data):\n","\n","        x, edge_index = data.x, data.edge_index\n","        attributes1 = x[:, :self.index_size].float()\n","        attributes2 = x[:, self.index_size:].float()\n","\n","        x = self.transform1(attributes1)+self.transform2(attributes2)\n","        x = self.conv1(x.float(), edge_index)\n","        x = F.relu(x)\n","        x = F.dropout(x, training=self.training)\n","        x = self.conv2(x, edge_index)\n","\n","        ypred = []\n","        for i in range(x.size(0) // self.sample_size):\n","            embedings = x[i * self.sample_size:(i+1) * self.sample_size, :]\n","            print(embedinds.shape)\n","            print(torch.matmul(embedings, self.matrix).shape)\n","            print(torch.matmul(torch.matmul(embedings, self.matrix), embedings.t()))\n","            ypred.append(self.linear(torch.matmul(torch.matmul(embedings, self.matrix), embedings.t()).unsqueeze(-1)).view(-1, 2))\n","        \n","        return torch.stack(ypred).view(-1,2)"],"id":"SW9_H-zWdRPl"},{"cell_type":"code","execution_count":null,"metadata":{"id":"CpFBRx0UdYLb"},"outputs":[],"source":["def train_gnn_model(gcn_input_size, gcn_hidden_size, gcn_output_size, batch_size, sample_size, cre_dataset, \n","                    feature_inp, num_epochs, backward_lim, dataset_train):\n","  \n","  feature = torch.tensor(feature_inp)\n","  model = gnn_network(feature.size(0), gcn_input_size, gcn_hidden_size,gcn_output_size,  batch_size, sample_size, cre_dataset)\n","  criterion = torch.nn.CrossEntropyLoss(ignore_index =-1)\n","  optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n","  all_features_index = torch.cat([feature, torch.Tensor(list(range(11,779))).long()],dim=0)\n","  dataset_train_selected_att = [Data(x = torch.index_select(data.x, 1, all_features_index), edge_index = data.edge_index, y = data.y) for data in dataset_train]\n","  loader = DataLoader(dataset_train_selected_att, batch_size = batch_size, shuffle=True)\n","\n","  for epoch in range(num_epochs):\n","    count = 0\n","    loss = 0\n","    for data in loader:\n","        pred_y = model(data)\n","        loss = loss + criterion(pred_y, data.y)\n","        # print(\"Pred y and data.y\", pred_y, data.y)\n","        # input()\n","        count = count + 1\n","        if(count == backward_lim):\n","            loss.backward()\n","            optimizer.step()\n","            print('epoch {}, loss {}'.format(epoch, loss.item()))\n","            optimizer.zero_grad()\n","            loss = 0\n","            count = 0 \n","    if(loss != 0):\n","        loss.backward()\n","        optimizer.step()\n","        print('epoch {}, loss {}'.format(epoch, loss.item()))\n","        optimizer.zero_grad()\n","  return model"],"id":"CpFBRx0UdYLb"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33987,"status":"ok","timestamp":1681386365086,"user":{"displayName":"Saba Shahsavari","userId":"04634919516474509529"},"user_tz":-210},"id":"OaBzkRhjByIx","outputId":"9b0f0a73-8bdd-4d56-b933-a41cf7092415"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading dataset ...\n","\n","Dataset info:  Number of nodes is: 9946\n","Number of edges is: 79389\n","Number of DNA of each cRE is: 768\n","Number of selected attributes of each cRE is: 11\n","\n","******************\n"]}],"source":["print(\"Loading dataset ...\\n\")\n","\n","cre_dataset = CreDataset(PATH_CRE_ATTRIBUTES, PATH_DNA, PATH_EDGE)\n","\n","print(\"Dataset info: \", cre_dataset)\n","print(\"******************\")\n","\n","\n","hyper_parameters = set_test_hyper_parameters()\n","\n","\n","cRE_edge_matrix_input = generate_positive_negative_matrix(hyper_parameters['portion_hidden_edges'], hyper_parameters['ratio_negative_to_positive'], \n","                                                            cre_dataset)\n","generate_graph_pic(cRE_edge_matrix_input, PATH_FOLDER + \"/Result/cRE-with zeros.pdf\")"],"id":"OaBzkRhjByIx"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21222,"status":"ok","timestamp":1681055032626,"user":{"displayName":"aryan sol","userId":"16168010549823356867"},"user_tz":-210},"id":"jehU-AIzaNI0","outputId":"570bca69-0478-43b3-c521-8197b3ca6397"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:09<00:00, 10.09it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","******************\n","Training the model ...\n","\n","epoch 0, loss 44.232200622558594\n","epoch 1, loss 33.100101470947266\n","epoch 2, loss 32.05465316772461\n","epoch 3, loss 27.8249454498291\n","epoch 4, loss 23.526899337768555\n","epoch 5, loss 22.63251495361328\n","epoch 6, loss 19.5361385345459\n","epoch 7, loss 18.142560958862305\n","epoch 8, loss 17.29340171813965\n","epoch 9, loss 15.367584228515625\n","epoch 10, loss 14.158831596374512\n","epoch 11, loss 12.57180404663086\n","epoch 12, loss 11.539145469665527\n","epoch 13, loss 9.917947769165039\n","epoch 14, loss 9.592373847961426\n","epoch 15, loss 8.993619918823242\n","epoch 16, loss 8.498170852661133\n","epoch 17, loss 7.944671154022217\n","epoch 18, loss 7.446652889251709\n","epoch 19, loss 7.1165666580200195\n","\n"," ********************\n","Testing the model ...\n","\n"]}],"source":["dataset_train, dataset_test = generate_subgraphs(cre_dataset, cRE_edge_matrix_input, hyper_parameters['radius'], hyper_parameters['random_size'],\n","                    hyper_parameters['neighbor_size'], hyper_parameters['number_of_sample_dataset'], hyper_parameters['ratio_test_size'])\n","\n","print(\"\\n\\n******************\")\n","print('Training the model ...\\n')\n","\n","model_cRE_edge_predictor = train_gnn_model(hyper_parameters['gcn_input_size'], hyper_parameters['gcn_hidden_size'],\n","                                            hyper_parameters['gcn_output_size'], hyper_parameters['batch_size'],\n","                                            hyper_parameters['neighbor_size'] + hyper_parameters['random_size'], cre_dataset, \n","                                            hyper_parameters['feature_selected'], hyper_parameters['number_of_epochs'],\n","                                            hyper_parameters['backward_lim'], dataset_train)\n","print(\"\\n ********************\")\n","print(\"Testing the model ...\\n\")"],"id":"jehU-AIzaNI0"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7247,"status":"ok","timestamp":1681055040927,"user":{"displayName":"aryan sol","userId":"16168010549823356867"},"user_tz":-210},"id":"HNf8impZbqr5","outputId":"1051f434-c4d2-45da-a30f-0d23379801cf"},"outputs":[{"name":"stdout","output_type":"stream","text":["f1 score is:  0.7260273249986813\n","\n","*********************\n","Evaluation test results ... \n","\n","True positive is:  543\n","False negative is:  181\n","New positive is:  59158\n","New negative is:  139598\n","False positive is:  153\n","True negative is:  367\n"]}],"source":["key_list=['A','B','C','D','E','F','G','H','I','J','K']\n","key_inp = \"\".join(key_list) \n","y_true, y_pred = test_model(hyper_parameters['feature_selected'], model_cRE_edge_predictor, hyper_parameters['batch_size'], key_inp, dataset_test)\n","\n","print(\"\\n*********************\")\n","print(\"Evaluation test results ... \\n\")\n","\n","evaluate_result(y_pred, y_true)"],"id":"HNf8impZbqr5"}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["SOH09vp0eWf7","z1EFkq__SAGe","6iFAFztrSGeM","S7pqB9lqTb4w","_TjBi-uv4E-m","6c5BoFNZEFD2","s3TQ7NNdFh5z","iJOM_pZOBvbk"],"provenance":[],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":5}